let's go good morning everyone good morning Mar is D and doesn't work that's all right we're going to use whatever we can use and we're going to see if we can solve press the light up button this the wrong python that py good so who's familiar with how to play press the light up button let's let's go we're pulling out all the stops we're using whatever we can good morning oh you got an ESRB rating you got a disclaimer for this stream am I just going to get disclaimers from now on it it is possible that there will be some marijuana smoking on this [Music] stream fuck twitch man oh No Just twitch though I live in California right wait that's ridiculous is fuck twitch against the ter I look I don't care if they ban me man it'll be good it'll be good sometimes you know it say it's like the guys who ban themselves from the casino man twitch banned me we'll have to you know here's the thing about twitch let's go um I I was looking yesterday I only get 50% of the money you guys pay to subscribe apparently Twitter you get 97% I'm going to switch my streaming to Twitter what's kick what is this knockof twitch how many of you how many of you would follow me to uh to to to this wait is this a joke wait this is a joke I like this I could be the biggest guy on kick all right all right all right well we we'll just say this we'll just say this hey kick reach out to me give me a deal I'll love to switch I love to switch twitch twitch got upset with me for smoking marijuana on stream uh and I would love to switch to kick and you know we we were going to rant about RL today but now we can be upset about this um I got to close the door Alex is sleeping and I don't know uh yeah no kick reach out to me give me a deal let's go go make me a make me a kick partner I like this I I like this I like this good I'm so happy I love to see you mocking the form um yeah yeah yeah yeah yeah you can you you you can hook me up with credit on a crypto gambling site I'm cool with that too as long as it's legal in the US we only do legal things in the US like smoke marijuana okay uh so yeah now that uh that upset me about twitch uh and I'm I'm happy to switch to to to kick uh but we're really just upset about reinforcement learning so we got to we got to stay focused this stream is going to be on on Twitch but if kick reaches out and they're like George we have a lucrative streaming deal for you I would love to switch um I I I was not aware twitch and if anyone from twitch sees this I I was not aware that partaking in in in inlegal conduct in California legal and wholesome conduct in California uh would get would get a would get a a a banner to come up on my stream warning people that I'm that I'm doing drugs like like I'm like I'm like I'm banging seven G rocks on this stream that's what people think twitch you see what's up so you know kick hook me up uh I would love to I would love to switch also twitch I I I never really thought about that but twitch is getting 50% of the money which is kind of insane uh so you know we know when I get banned they'll be not really because of the the marijuana incident it will be because I'm speaking truth to power uh you know I'm surprised I haven't gotten banned from LinkedIn yet but um Yu I appreciate I appreciate the the sub gifting but we are going to move to kick uh no I mean this is this is how many of you would subscribe to is the same thing can you subscribe to me over on Kick let's switch over right now just subscribe for six months no twitch twitch uh this is this is the uh the marijuana incident is is really the uh the the last draw um I am I want uh I want a moreare split uh 50% doesn't cut it for me anymore I want I want the banner lifted from my uh page and I want the uh I want more than 50% Revenue uh I you know I I look I look look like we do this for fun but now now now you're branding me as as a drug user and now we got a problem so uh that's that's what I'm saying uh twitch you can you can reach out to me today kick you can reach out to me today uh this is this is going to uh yeah yeah we're using leverage and we're negotiating all right I get like a thousand viewers on the stream man thousand concurrence that's pretty good what else I could be like the biggest streamer on kick you got you guys got anyone with a thousand concurrence more more more more more well okay I'm not beating India Support over here um you can beat this girl I I just I can't beat this see what we got oh oh content been marked as I like I like it a lot it's very good see now twitch is going to Mark right very nice mve yeah I'm looking small M very good very good you don't like it I think I'm just look at this I'm very full I ate too much you want to eat something too much today I'm very full it goes BR 10 hours I love this maybe 12 hour yeah I know how this noodles yeah please eat noodle no no no I'm going to go to home and why no I need the yeah uh some of the kick streamers are banned on Twitch no we're we're we're it is 2024 and we're speaking out for the rights of everybody uh if twitch bans me Twitch bans me that's that's their choice uh but no twitch we need a better deal we need a we need a better deal 50% is not cutting it anymore man uh and you know if I make a tweet dising twitch and praise and kick they'll hook me up today we're not ready to go that far we're not ready to go that far yet um I I don't I don't I don't know enough about kick they have they have a link tree can I can I subscribe to their only fans um you will definitely not get a better deal from twitch or from kick twitch warned me that I might be intoxicated swearing a lot and potential nicotine use no no no we're definitely not supporting Google look we're really only on Twitch for legacy reasons what's Justin oh Justin TV kick is kind of shady with their deals yeah they look they look kind of sh Shady uh no I think we should TR stream on X I mean the problem is if I stream on Kick no one's actually going to find my stream but if if I stream is real George Hots on X does this work and then I think can I gate my streams behind subscribing to me yeah I did one stream on on Twitter before I had to get like a media account I I don't know I don't know if elon's fixed this stuff why not multi stream X doesn't have chat you'd rather watch on Twitter uh I don't know but all I'm saying twitch is we're going to we're going to need a better deal uh we're going to need a better deal and we're going to need we're going to need that uh that content warning removed uh and you know if you're not ready to do those things you're going to have to ban me that's that's that's how it's going to go um X added chat on desktop let's see well well let's see let's let's let's talk about my negotiating power is science and technology uh on the front page I have more viewers than most of the people on the front page right now I have more viewers than that I have the same same number of viewers as that I can't compete with these I don't know what tarov is uh watch hours are very important for negotiation do I stream for money I get money for it and I think I should be getting double that's right oh look look look look look I'm not I'm not here to be difficult but you know you you you flagged My Stream as as as being uh as being uh as being adult mature whatever and you're clearly watching my stream at twitch and now that you're watching I want a better deal that's all I'm saying all right no I don't think they they don't they don't they don't give 50/50 to to Big names all right I like all right so I'm I'm the 7,000 top streamer on Twitch have a lot of followers wow people have wasted 330 hours on this junk remember that time I showed my I showed my Revenue you want to do that again big names get 7030 okay i' i' uh no no no no no no no I'm not going to stream more regularly I stream regularly on the weekends now and if that's not good enough I I'll wait for my kick deal to come in man no uh we we we might switch to X well we lost followers on that one that's kind of wild no I I applied once to be a partner and I was rejected from it and the whole thing was the whole thing was not uh you know I really didn't like it it it was not a uh like it was not like if you meet these requirements you you get this it's a backroom deal thing they just decide if they like you or not uh which you know kind of upsets me on Twitch there's no better streamers in T and programming than me I believe that um yeah so you know twitch you triggered me uh we're going to need a better deal uh we're going to need we're going to need a better deal uh yeah if we like you you get a deal and you know I don't like that sort of look I'm fine if there was like an actual rule like I had to set a schedule for streaming I had to I had to actually do it um want to see my revenues what what what what what can we do that's mildly on the edge right now to like get banned because you know getting getting banned is is uh here's my Revenue it's 30 days I made I made $88,000 you know if if the split was more fair I would have made more um I'm not not gay it's true not that there's anything wrong with that uh hot tub stream what else what what else can we do that's like kind of on the edge to to to to to get this to get this ball rolling let's Google best ways to get banned on Twitch all right you know what no no no we're going to perplexity that all right no to avoid getting bad a stupid AI you see this is this is the problem with AI okay whoa whoa whoa whoa whoa first off marijuana is a legal substance what is hate speech I do hate uh who do I really hate the professional managerial class weapons oh where's my where's my uh where's my where's my here oh yeah yeah yeah look at that look at that I got a box cutter uh what else we got nudity sexual acts that doesn't sound that doesn't sound good oh derogatory language relating to sexual orientation no that's that's that's that's you you know we we're supportive of all the lgbtq rainbows um we're just not supportive of college administrators inciting violence h no no we don't like that making fun of medical conditions oo oh man what medical condition can we make fun of oh yeah uh disabilities or other sensitive topics so engaging in illegal activities displaying copyrighted material without permission or guys of promoting gambling streams what what gambling streams can we promote I don't even know any of them if I just say them is that promoting a gambling stream oh you I you how I feel about fat people guys being fat is a choice um that is that is definitely true your weight is your choice stake.com is banned guys go watch some gambling streams um you want to get baned stream a movie no I wouldn't want to violate a Studio's copyright they're just out there trying to make money uh wait can I really not talk shit about fat people I'm sure we've talked shit about fat people of all the things you know of all the things look you make fun of someone for their race you make fun of someone for their height you don't do that but you make fun of someone cuz they're fat that is a choice man you choose what you put in your body remember yesterday when I ate that cinnamon roll I regretted it all right and that was a choice I made and it was a poor choice and fat people make poor choices every day and that's why they're fat a segment called fat watch harming yourself on stream some of these things are fucked up we don't want to do that man we we want to do we want to do normal things um would not like that hay is being uh made fun of sexual acts is nice has to be personally insulting yeah that's just not my style guys we don't we don't want to insult people um there are two G well no she's not really gender Spectrum guys um masturbate on stream nobody wants to see that uh where where where's it where's the deck of cards does anyone want to play blackjack right now does would anyone like to play me at a game of blackjack selling meta stocks is a choice that's true gender gender is actually a spectrum sex gender distinct I mostly agree with this stuff um all right all right let's let's let's get back to RL you know we can't just we can't just have a stream there meta and 800 viewers now could report me yes please no I don't know should you actually report me what's going to what's going to get me a twitch dealer banned we got to we got to accelerate this you know what I'm saying why am I not logged into anything change the title to please ban me start around Andrew T A let's see let's see a you know the funny let's see Andrew t uh the funny thing about poor people is they should just stop being poor me I have seven Ferraris and I rape girls in Romania and it's okay cuz I'm Andrew Tate okay and they tried to put me in jail once and it didn't work that's because I'm richer than you and better looking than you I don't know I don't actually not like Andrew Tate I think he's all right top G top G we we don't I don't actually hate Andrew Tate I think it's okay okay so remember yesterday we coded this great game press the light up button today we're going to try to use stable baselines to solve it all right stable baselines three we're pulling out all the stops we're not using tiny grad here we're using we're using stable baselines all right include [Music] I don't find the AI coding to be very helpful how does this work okay we need it to be a gym environment uh it's mostly a gym [Music] environment should probably pre- select all the states here is see I don't like how do I pass in parameters to a game um I we can just like I I guess just putting them here oh wait I can just pass parameters after that one that's kind of cool cool um render modes that are supported or just none and we don't need render FPS uh [Music] state so we have to uh to get the observation self. size we to do self. state sub self. right see it's nice to be H I'm thinking I'm thinking clearly in the morning which is good uh so we have to put the observation space what am I getting none there oh no stay off twitch don't stay off the reefer man twitch is upset with my weed smoking then you know what uh which is GNA go off twitch we're GNA have to do it it's you know it's sad but it's gonna have to happen is this inclusive or exclusive great don't give me any docks one above the largest signed integer okay good press the light up button is working let see if we can learn press the light up button has no OBS okay well that's fine no one ever said spun is going to make me more productive man index one is out of bounds with a step size of one um yeah that's true because the game is over none type doesn't support object environment for I don't know about this non type object does not support item assignment doesn't look useful all right perplexity let's go not returning any info information info dictionary oh is that what this one has to be okay let's see what the return is from Step info here get info or do we have to do info we need a dict great I think it's actually working maybe I don't know uh okay oh I can do do render that's pretty cool except that render doesn't obviously work on mine but is you tried to render but the render mode must be defined when initializing the environment must be human or RGB array not none I see for okay so we grew this game yesterday it's called press the light up button one of the buttons lights up you have to press it it just basically uh put the state in the action okay this thing actually looks like it gets very good I press the light up button that's cool uh let's try press the light up button with size 10 something we could not solve yesterday wait it solves it okay let's try a longer game um um this truncated isn't [Music] working why is it not finishing the game okay when I have a game length of two for some reason it doesn't finish the game this is really dumb oh cuz we're resetting it every time okay well that's not right okay cool uh so yeah it looks like it learns this game pretty quickly actually all right stable baselines 3 just works uh let's give it a big size and we'll give it a game length of 10 let's see if we can get 10 reward didn't learn that fast let let play for 20,000 time steps okay I learned how to play great uh so stable baselines Works um what if we just change our environment I love this game by the way so okay do every understand how this game is played uh this is the lights and this is light 11 so you press button 11 right this is the lights this is 0 1 2 3 4 you press button four you get a reward uh light 13 light one light eight you press the lighter button and it goes on for for 10 oh let's see if it can learn in hard mode um in hard mode okay it can't learn in hard mode um in hard mode the policy changes uh for each step so for the next step you have to do something different no it doesn't look like it learns hard mode at all as a chance hard mode just doesn't work um so now we can play the game ourselves okay uh that is 01 two three h and let's also play it with a smaller size let's also play it with four so you just have to add wait what I told it size four there we go uh so that's the answer to that is 0 1 2 3 uh int object is not subscriptable okay oh it's a vector environment I see uh so the answer is three the answer to this one is also three yeah see because you have to do that no but that somehow ended the okay now there was just a bug uh action plus self. step Nom mod self. siiz that should be right one the answer to that is episod one no it somehow thinks the target was two try three there okay three is wrong okay so you have to you have to put in the action like one before right so for this one the action's also going to be three yeah okay so that's right so hard mode does work see if we can learn it with so my guess is this policy doesn't support anything temporal it doesn't seem to be able to learn the game on hard mode but the game on easy mode is just the game on easy Mode's very straightforward it learns it very fast okay let's see if it can learn lunar lander see look at how nicely it progresses oh it's beautiful I just wish I could set render [Music] mode okay let's figure out how to set render mode they had some like render you can type so this is using stable Baseline this is like the most common RL can render him it doesn't [Music] work you have to create it with render mode human so that's the Baseline posy what I'm learning is that not going to do R mode human no now it's now it's doing render mode human to learn I don't want that should also be able to register press the light up button with Jim um forget v m m reset M step trunk cated you have passed a tuple to the predict function instead of a numpy array or dicked have I done that did I do that uh it all works nicely if you're playing in stable baselines setting a model array with a sequence the detected shape was whatever we got to figure out how to fix this to actually switch to rendering human mode get M return the current environment VM render human this should work oh render mode RGB array okay that for some reason worked but now what happens when I learn here okay that for some reason Works let's let it learn for a bit see look how the reward goes down I mean goes up I mean okay so RL works if you use stable baselines well sort of that's pretty [Music] good all right so now we reimplement this in tiny grad well thees stable Baseline support uh decision Transformers we've also learned that press the light up button should definitely uh work because it learns it really fast even with a large size and a large game I didn't learn that one that fast but okay let's register this with Jim how do I register a game with Jim well we tried decision Transformers all day yesterday and they didn't work so you know there's just 7 million places there could be bugs in tiny grat there could be bugs in my implementation there could be bugs anywhere uh and we didn't really have very good tools to introspect uh which is what upset me okay um I don't know I don't know how to register it okay I push this to geohot Dum RL um um I think they built a good I think another reason this thing is good is they built a good uh Vector explained variance Advantage actor critic so much code does this thing reliably solv C say the thing it's like okay the tiny gr example for this is better that one didn't work okay it doesn't work every time maybe theirs is more stable not time it learned see the reward you probably can't see that oh yeah don't go off the side no I mean whatever they wrote in this seems to just be much more robust than what I have okay let's find the stable baselines decision Transformer in the current weeks we'll be extending the ecosystem okay did you do that what was I reading what was I reading last night I was carefully reading the decision Transformer paper last night um I have it right here on my folding phone [Music] [Music] for [Music] okay let's get decision Transformers working on press the light up button stable baselines Jacks sounds fast what's tqc o truncated quantile critics I was reading about this so they they wrote a really nice wrapper for vectorized environments first uh which which is smart like a lot of this a lot of what was struggling with yesterday was just that I don't have any uh so I watched these uh like like I watch I watch this and it's it's a video about like the uh the roads in Congo further on there's another pothole which has been there for so long that the drivers have dubbed it the ancestors see just that little clip that that pothole that's been there so long the drivers have dubbed at the ancestor um and you realize that what these people should do is build infrastructure they need to fix their Road and I watch these these these these documentaries and it's just over and over again people putting in 10x more more work than they should put in because everything needs to be done right now they're kind of thrashing about um and it's the exact same thing in code so yesterday what I'm doing is I'm struggling with could there be bugs in tiny grad could there be bugs in my Transformer trainer could there be bugs in my RL implementation could there be bugs in my game uh you just you just don't know um where the where the bugs are and what you have to do is you have to carefully build infrastructure or you'll sit around and you'll thrash the whole time uh say it's nice to have things like Sable baselines which guarantee you at least that like someone's looked at this and someone's maybe it even has some CI testing and and that sort of stuff um but yeah know this is the best documentary deadliest Journeys oh fire let's get some more coffee and then let's find an implementation of uh decision Transformers and see if we can make that work on press the light up button and I will leave you while I go get coffee with this that's too many times St it's going to take too long see seems like a [Music] number so what makes RL a lot more challenging than than supervised learning is you don't have a fixed spell that word right I don't think so oh okay this is what we learned is it going to land I mean this is clearly showing even though it's not Landing it's clearly showing some intelligence it's keeping the thing in one place good recovery it hasn't figured out Landing yet but I didn't write this this is this is just stable baselines uh no I was also I was just frustrated on stream yesterday about if you saw my tweet we're going to we're going to Thanos snap on Tiny grad we're deleting a lot of it we're deleting all the back ends people don't really use we're focusing on the things that actually need to work well um wow look at that it's almost like it's going to land it does too many lines uh okay no 100,000 no iOS you you've said many distracting things so you're you're done talking I think you're done talking you time out someone for the rest of the Stream okay let's see if we can get the ja one working my might be faster it's just going to work so might be easier to read since it's new [Music] well we TR it doesn't have [Music] a2c I like how their reward continually goes up Bas if I just change this to sack all right I I remember what we were trying to do now just try that no we can't use sack because it only is cuz it's disc great Advantage actor critic they're also playing a lot more games than I was like in order to learn this it's playing 10,000 it's playing 20,000 games so I wonder if it's just that if my thing was just a lot slower how many update steps do they take this is really nice uh okay so they're taking yeah they're taking like an order of magnitude more update steps than I was um I left it on for about an hour after the stream it didn't work I'm not sure it can solve in Orlando but it might very well solve um solve this okay decision Transformer repo let's just try this start there how do I use this thing experiment module name gy I don't know about this is there like a is there a useful decision Transformers repo that I can just check out here let's see this hugging face example decision transform what custom data collator takes about 40 minutes to train in a collab [Music] Network okay uh let's go let's go over to Tiny grad for a bit and let's modify beautiful cart Pole let's modify beautiful card Pole to solve uh press the light up button I'm going to copy the press the light up button game this render is stupid let's create a new okay this is wrong no go here got this let's create a new folder called RL I guess that was kind of a stupid repo I don't know if we need a hold repo for that um render function G so we we create the environment down here with a is there some way to like convert the environment like I I I recreate it down here so that we can actually see it rendered yeah works on on Ubuntu yeah let's let's make this thing reliable let's figure out what tricks they're using in stable baselines let's make this reliable and then let's see what we can replace with a decision Transformer and we'll get it to register too okay so this is this is Tiny grabs cartpole solver it's called Beautiful cart Poole um it works maybe 50% of the time 75% of the time let's get this to register examples. RL light button we'll just call the ID uh the second element returned by m reset was not a dictionary oh this is supposed to be info as well I think okay so it's getting one reward test reward 1.0 okay I learned that very fast uh let's let's make the size 16 and the game length all right when I give it a bigger size it does not learn what if I just give it game length 10 does it learn on that okay it very quickly learns to get 10 uh what if I give it the size two does it learn that size four no it struggles a lot more interesting I wonder if I have a bug like how come it can't learn with four but it learns fine with does it learn with three yeah it learns with three but it struggles a lot you see the test reward you see this does everyone understand the press the light up button game Che so it looks like yeah for some reason it doesn't work once I make the size big but the uh look like it doesn't work at all but this one does so what am I doing wrong also think that my model might be a little small let's take a look at stable baselines as model what C RL debugging let's see what we got why is debugging RL so hard yeah yeah oh here we go look look at these nice little uh oh wait I love this no I'm sure stable Bas lines is good I'm I'm curious why mine broke when I switched to a [Music] uh that [Music] so I think that again they're just doing a lot more steps than me we're just not doing many what these are my hyper parameters all right we're sitting here we're tweaking hyper parameters again not getting reward where's my sampling here Tod do what what's the temperature here oh we need to add a temperature also is this even right where my soft Max [Music] okay evaluate clearly takes the AR Max so that's fine uh X assumes that there logits on the output which is fine but we have a bug where we're not okay so if if you're following along with this this is beautiful cart pole in uh in tiny grad we're going to try to get a reliable uh a2c thing and then figure out where we have to change it okay so you divide the logits by temperature you don't it's not really logits that you want there this is labeled wrong in gbt so I think we first do the X then we divide it by the temperature then before we do multinomial we do a soft Max and I wonder if that's part of the problem I mean like multinomial shouldn't even work if it's not normalized so I don't understand still doesn't work wonder if lands cple more reliably now though so we're extracting the hyper parameters to here [Music] uh we change that 30A why am I that's the actor not the critic now this is unstable oh multinomial does the normalization I see really but what kind oh it does normalization but that's not the same thing as softmax right like it yeah it's not the same um God there's so many hyper parameters here called gamma [Music] does this solve the game with size four four at least sometime randomly gets reward doesn't solve it so we have to still have bugs now the problem is not stop stop stop stop stop the problem is not grid search or anything like that the problem is probably not even hyper primers the problem is probably bugs even this it doesn't say stably soft it seems like I broke something by adding soft Max and temperature I have double exp it's x. divt multinomial no I don't think so so this exp is because I want the model to learn Logics but I agree that that softmax makes it worse in practice well then what's the loss that's even weirder like we we can't not have a we can't not have a uh okay that solves it right away and if I add a soft Max it breaks it but we can't not have a I mean you think it's that all that seems to work too just remove that EXP solves it fast let's change the size to four solve that no can't solve it with four okay the model outputs logic J how do I select an action temperature so I also have log softmax too I mean that looks like that mostly but I call it log dist here and I do an exp here well okay then that's on do an XP too like I don't think this is right I think I have to do a softmax and I think this is exactly why it doesn't work with more than two actions see this is so complicated you do little things you do exp instead of soft Max and that just breaks everything one of the best things is still the open AI spinning up in deep RL see I bet I bet you that's I bet you that's the bug what is a soft Max Max Dev why do I have tan H feeding into a softmax I don't oh I have a log softmax here already oh okay well then this was right to just be X but I don't have a temperature so this was right I think CU I have a log soft Max here still can't learn it like there's got to be something wrong right because it learns it with two instantly so reward 10 but if I make the game size four it doesn't learn it anymore oh that time it learned it unreliable let's get that to be reliable there's got to be some there's got to be some bug I have a test for log softmax in in tiny guys right yeah and log softmax does what I think it does apply a soft Max followed by a logarithm wait that just might not be what I think it is I think it is okay why doesn't this work with more than two [Music] outputs so much potential for bugs let's take a look at this and see if it's a proper probability distribution should be okay that looks like a pretty legit probability distribution meaning they sum to one why is it not capable of learning once I set the size to if I set the size to two instant solve if I set the size to three solves but slow not even always yeah there's definitely a bug the entropy loss not these things I maybe this like a broadcasting bug why would something so reliably work with two and then break almost entirely when it goes to three see sometime it gets lucky and gets it let's go somewhere it just can't get lucky let's go to Five nothing for those valid log distributions yeah they look valid it's not printing multiple times because of the jet let's confirm the J's not somehow breaking something it's still broken it's just slow um is it a ranging issue can't see the output but you may need to Res no no it's it's it's cross entropy I just I don't know if this is [Music] right the Mas and manage calculation might have a mismatch it added torch I guess I don't really understand why I do that sum across there it's maybe it's these sums that are wrong what is do minimum oh minimum should be a local minimum between that and that can we sum across the action to Di menion and then we take the mean across the whole thing is that not what we want there's so much like like look at this tiny function there's so much potential for bugs here and I don't know how anyone else gets any of these other things to work because they're so big and complicated where's the PO function in here policy loss Advantage actor critic okay this doesn't even look like it uses po then MSE loss entropy is none you just take the negative mean of the log probs why do I do this why is mine more complicated they have a value coefficient okay it looks like they don't even have P normalize advantages roll out data. advantages okay advantage reward minus the prediction from the value am I supposed to detach that no okay I detach the whole Advantage here I multiply the advantage times the log prob though again why do I take an exp there can I just do a mean overall they they don't bother to sum across that axis which might be totally fine this just doesn't work [Music] compare the output of the one that works yeah but like there's so much difference I can't even find where their stuff is do I not detach that I mean that's probably a bug for okay why can I not write action loss equals log dis time M advantage. mean you have no idea you're talking about um some input tensor not found oh that's because I'm passing an all log dis and not using it that's just a jit buug does that work to solve size two the 10 H has nothing to do with that the 10 H is in an intermediate layer this doesn't solve it anymore oh well that's because I need a minus sign here oh okay if I don't use po it doesn't solve it how how does that matter oh well is the bigger problem that I didn't do an exp it's probably the bigger problem okay that solves it it just takes longer and we have another hyperparameter here called train steps okay that seems to solve it and behave how we think it should let's take a look at these Mas advantages and see if they make sense Advantage we subtract the the estimator the value you see I wasn't detaching there see that was probably a bug okay so we reshape the reward to match the size of the value that seems right bump the size to four see if it gets solved interesting it gets solved it just takes a while is it worse with PPR interesting it's worse with PPM do I have a bug in my PP or we just got lucky and unlucky that eventually solves it I worry that things like this might be tiny grad bugs too like who's tested if clip really works and minimum really works who's tested if detach really works [Music] for oh look we got some big rewards there random we're watching random numbers again now we can't even solve we can't even reliably solve this with big sets of uh we we can't even reliably solve press the light up button with big sets we let this finish and we'll watch lunarlander Crash okay our best landing yet um so I wonder if the difference between these things is actually the fact that I have a sum there no it's weird it's weird it's it's worse with PPR can this one solve size eight or does it just get a little bit better okay it solves it interesting okay okay this is good progress um maybe the sum is actually good also be nice if this were faster actually need that line in the jit that checks that thing here look reward 10 uh so I'll put the jit back on and I'll get rid of this summon poens is not found this we don't really need okay you don't have to use the input tensors okay so this is reliably now solving with size eight uh the thing it's just when I put po back on it breaks okay good good good this is this is good progress why does po suck detach can't be wrong I don't think so I mean we can always do this too okay with po no solve yeah probably there a bug and PPR that would make sense po should actually like not do anything most of the time which makes me think that it seems to reliably solve it now well okay kind of it's a lot faster without P too sof okay let's carefully read Po and see if I wrote this right uh old policy divided by new policy policy so we're effectively doing that when we subtract in log space then we take the X um it's not fucked up if he deleted the nice messages okay it's not nice to go off topic it's nice to what's nice is uh is is correctness ioso I I don't know if you're trying to be helpful or not but that's not helpful and I'll explain why and Gerald Dam you get timed out uh that's not helpful because the the problem is not is po or trpo better the problem is why is my uh po not working sure looks like that's what I wrote wait hang on no maybe not no oh wait this is wrong I'm multiplying there by masked advantage and then I'm putting that into the clip no this is wrong okay so this one gets clipped I'm multiplying it by m Advantage twice that shouldn't be there then we have another one this shouldn't be ratios minimum this should be ratios times mass Advantage minimum okay let's say look like it works better but that's clearly wrong right still broken found a good a good basic po that doesn't work either but it's clearly wrong what I had right so complex see that solves it okay calculate the ratio current log probs batch log probs compute the surrogate losses he also normalizes the actions which is kind of interesting but okay compute the S okay so I mean let's just write the right thing there was clearly a bug here unclipped ratio equals we got to stop trying to be clever in writing things on one line that's really hurting us now we want the minimum they're calling AK is the masked Advantage batch rtgs minus V detach I think that's right check critic loss I don't think that's wrong I mean remember this works if I disable the PO lines so right now we're just we just get no reward that's crazy we fixed the bug and it still sucks but this is okay so we do vanilla policy gradient it solves it see it's solved after 100 episodes we found two bugs so far 0.2 found one bug with the detach and one bug with how we were Computing this and the worst part is it's still solved cple let's go back to card pole for a minute soft where's vanilla policy gradient doesn't solve it well it solves it it just takes longer and then is unstable okay that solves carple faster and none of those bugs actually seem to matter know that bug was really bad wait did I get my signs right let's check to make sure we're not doing any silent broadcasting well those are the same shape we definitely take the minimum and then do this subtraction it doesn't actually matter if we do the mean before or after those parentheses are useless they have a separate Optimizer for the critic and the uh then the and the actor I don't think that matters I don't think that matters The covariance Matrix no it didn't solve it is this really right we had an obvious bug and it didn't affect anything just infuriating okay compute the current log probs is an open source test Suite I'm sure there's many things like that but that's the problem there's many things like that we just subtract the logs it's just the same blah blah blah BL blah blah blah gradient descent easier okay great um you sub with your hard-earned money bro you just got banned with your hard-earned money that's right that's right that's right I don't know what you thought this was do you think this was a democracy huh there's a lot of zeros there also looks like broadcasting and broken we'll go back to the game size 4 just it get smaller [Music] interesting how are there so many zeros oh I'm looking at clip ratio I want to look at ratios let's look at ratios they're all like kind of in the right range they're all like kind of near one why is this one always one that looks broken that looks really broken most luck with this implementation let's take a look okay TF minimum train batch postprocessing advantages times clip by value yeah okay that's that's what I have how did we have major bugs in this and it just affected [Music] nothing okay the they're doing the square the other way but that doesn't matter right x - y^2 = y - x^2 true or false 7 - 4 = 3 4 - 7 = -3 that's the same square okay I think that's right I don't know what this ignore the value function is there's bugs this shit's impossible guys okay look we're writing code that looks looks cleaner we have unclipped ratio and clipped ratio and then we have action loss these ratios look approximately right I'm asking you to learn the game [Music] um okay we're supposed to return a numpy let's return a numpy let's that war it's not learning it's so weird though is it manages to learn okay vanilla policy gradient works it also manages to learn the uh card pole but it can't learn press the light up button with the wait what that just learned okay I work now we're in hyper parameter land let's ra slow the learning rate okay you change the learning rate it seems like it's learning Now sort of no never mind doesn't learn anymore at least I'm pretty confident this is correct I'm glad that we fixed these bugs let's give it a try on lunar lander also raise the hidden State size maybe I'm just trying things this is stupid no this there's no this is not there's something deeply wrong selected action none of this is attached right no all not wrong solv it okay all right at least at least it's solving it sometimes now um and we fixed a serious bug so I'm happy about that we moved all the hyper parameters up to hyper parameter land let's see if we can learn how to land the lunar lander probably too ambitious okay well so close let's see how reliably we're learning card Pole now okay good oo ballsy bro ballsy somehow didn't get 500 on that I don't know okay this looks way more reliable than it used to be oh no no it went on stable never mind I spoke too soon um I less learning rate more hidden State the problem might also be no the problem might also be hard to learn let's take a look at a2c and see if it's a cuz this might not be enough layers this problem is like an exor problem actually wait no it's not no it's only an exor problem if I have to be able to play the game both ways um okay you say the one that works is the one that normalizes the advantages okay solves cpall like it gets fast after a bit which is cool when it's done filling up the replay buffer for [Music] I still think something's wrong with my PO because this is doing better who knows maybe these clips don't work work like if anything's broken in tiny grad's gradient engine it's this torches had problems with this kind of stuff too um or it's not really torch it's like you don't exactly know what you're doing you don't know what's getting uh where the gradients are flowing where the gradients aren't flowing for okay well it's great we're not going to you don't get a choice you have to use PPL [Music] so actually I have a lot of questions about the use of tan as well oh no we just got lucky that time it has nothing to do with anything we just got we just got good initialization seems worse with a larger hidden state I'm not swapping tiny grad there's no point then when you just swap it all out for baselines again this is a deep misunderstanding of what we do on this stream okay we don't solve the problem nobody cares about solving the problem now we can write some tests to see if the gradients match that's different we fixed like a major bug and nothing got fixed deep misunderstanding of my suggestion time out bro that's right Iron Fist this morning now we're not adding more crap why does this not solve why does it solve with vpg and not with well now probably doesn't even solve with vpg anymore because we got rid of tan H which actually turned out to be great are they using three layer networks or two layer networks let's go back to that that tensorflow one you said you had luck with this one looked kind of clean okay ppf policy great but of course it doesn't include any uh do include the the the catalog oh my God see this is just like what am I even reading here no Tage is in the middle Tage is stupid okay on policy algorithm actor critic policy okay here we go actor critic policy observation space that space that sounds good feature extractor how does anyone write any of this stuff man carefully initialize the stupid create the layer that represents the distributions okay latent dim to action DM and then they like MLP extractor let's see how many layers is this oh here we go it's an option that you can find here how do we get the layer sizes where do the layer sizes come from oh there's a value function coefficient that's interesting okay we have a gamma whoa look at that learning rate whoa 7 e minus 4 that's that's crazy that's a crazy small learning rate oh they're using RMS prop oh this is what I mean like why is it RMS CR and not atam why do they have a point explained variance that's cool oh and then they have a FP graad Norm great so how do I get the size of the actual Nets is that going to be something we can find all right so it looks like they have two that are 60 4 all right so let's add another layer and their hidden State size is 64 I mean there's a chance it's this these functions are not these functions are a little Annoying for these things to learn I think so but I don't know that actually probably doesn't make any sense but whatever we're adding another layer anyway don't add more layers until you made it work with the other layers yeah I know I probably shouldn't do this but we're doing it anyway so you know that's right um okay now we've matched the other one now we learned nothing but remember that their learning rate was a lot smaller than ours we were just like getting like lucky on cardfall oh there we go okay is it reli ible okay two in a row can we go for three let's go for three three three three three okay oh that's using vpg now let's put po back in and see if that works Sol fast okay good good good good good all right two let's see oh oh boys oh yeah we needed a three layer Network oh come on no no no no you were so good you were so good no it didn't work that time why is po more unstable no we're not fixing the random C that does nothing does it work with cart ball yeah okay look it learns much slower now but it seems more reliable maybe I don't know I'm just making excuses for an underperforming Network don't do it don't go off the oh 500 exactly all right can you solve lunar lander no of course you can't solve Lun Lander that's way too hard great we're back to that guy it's interesting that it prefers that I still think there might be some bug in my PO still B for a while look at their rewards they're going almost positive now look at those rewards oh no no it lost that it lost that oh it was so good well this is the problem with vpg it's really unstable okay we had it good for a little bit it might have made a landing on the moon but we'll never know cuz those records were lost no now it's just in it's in it's in junk land oh great let's try with size eight the other one worked with size a right let's go back to Dum RL uh we can also now do whatever um how come they have a learning rate that's much smaller and how come they're using RMS prop what batch sides are they using N M's might be using a much smaller batch size okay this just isn't solving now does vpg solve this one no how this is so simple how does that not work shut the search tab oh this search tab I like the search tab I'll make it bigger no it's a little too big okay again like we're not learning we're just it's not learning how is it not learning when I give it lots of options it forgets how to learn it gets reward sometimes change learning rate to their number can try that I actually worry that it's wrong in the other direction okay now it just doesn't solve this anymore because I raised the size to eight okay I mean we we were too optimistic with four okay let's go back here and see if this actually reliably solves this solves [Music] 16 yeah look it gets reward right away to be fair we're doing a lot less total times depths than them so let's try like total times STS of 100 and see if we get it that seems to work better it solves it very fast so we're just we just have something wrong still we are going to find the book okay maybe we should start thinking about what the the thing is there really is nothing to learn in the value function we're not rewriting this in P torch you like again the point of the stream you want to write a test do do you think that the problem is do you think that the problem is that the train step is not Genera the right uh is not generating the right gradients we can do that if you want to test that fine we'll write in by torch is that close enough to pie torch it that just works oh it's so hard to write things in [Music] pytorch just sounds so painful sounds painful and it sounds like we're not going to learn anything because the bugs are just going to be different we can check the gradients that's fine you know where I worry the problem is what are these being initialized to [Music] okay this is what it looks like there's so much potential for bugs here okay I don't I just don't want to do this you know if all right let's just convert this to pie torch if it just works in pie torch man I'm I'm just I'm just that's it that's it if it works in pie chch we're just shutting tiny Corp down this is that that'd be just total bullshit f why I hate P critic has no attribute perams how do I do this I'm sure co-pilot can do this for me get pams parameters no I this isn't Matt have to be the same D type but got long and Float kind of a Bugg and light up button log soft Max expected wrong file multinomial expected parameter num samples brand in missing one person parameter size branded an argument size must be a tuple of in not an in tensors can't be realize because this is torch probability tensor contains Nan in or okay PCH Wow first off holy shit that's fast second off why is it n wait that's insanely fast we should we should give up just that's insanely fast yeah it's the same okay so pie torch has n and also doesn't solve the problem P seems just less numerically stable it has n okay great you guys Happy P torch is less numerically stable and still has zero [Music] reward though that is a little concerning that we are on the edge of numeric stability no it's faster it's it's a lot faster no I look there are tiny grad's not fast at things like this P George is um but it didn't help with the reward and it's actually unstable with a learning rate with the same learning rate I'm using in tiny grad so I mean you watch my translation do I have bugs in it except for removing the realizes and removing the jit and the speed isn't that good no RL doesn't work I mean that's true if if that just worked we would have just shut tiny cor down maybe that's why they had 70 minus 4 yeah I mean all right let's go to 70 minus 4 and let's train it for 10,000 EPO let's go okay let's go back to things that we know actually work let's go to cple cple work okay carple works great so my translation's correct wow that's fast though can we go back to Tiny grat now that we know there's not there's not bugs would it be great to have an automatic pie torch converter no no like the whole thing is frustrating right like of course in many aspects tiny grad is in no way as good as pytorch um but it will be better eventually and the the problem with this is you just kind of have to have faith in that I can make some arguments I could make some I I could make some some reasons uh but mostly it's just a matter of faith uh and that's all that's all starting your company is right if if you start a company and you're like we're going to compete with Google and you you have your first thing and of course you know it's it's not as good as Google it's it's very quick to get demoralized um the the the the advantage and hopefully what ends up happening is that the very simple underlying stuff of tiny grad makes it very easy to make progress in ways torch can't uh but okay we we translated it to P torch and we see the exact same behavior uh I mean it is true that if we're worried about places where okay something you can do something that does someone want to write this here you guys for someone can write this so I wouldn't translate at the API layer I would transl at the um mlops layer there's this thing in tiny gr called mlops and if you just Implement these things in pytorch if you implement versions of this in pytorch uh you can test the automatic differentiation wow that's so fast though like it's equally shitty and can we seriously talk about the fact that tiny gr is more numerically stable now why we're on the edge of numeric stability in the first place well that I don't know which is another like so fast so fast all does tiny gr handle mismatch of D types yeah it'll just do the right thing it behaves identically in pi torch and Tiny grad so hopefully we can rule out tiny gred bugs now the OBS returned by the reset method was expecting the numpy D type to be in 64 the OBS return by reset is not within the observation space H oh here okay I don't think any of that stuff's the problem uh yeah tiny grad has has the same upcasting Wheels pretty much as Jack as Jack's it is so much faster to just test this in P torch though it's so fast learn nothing quickly with reinforcement learning it's not the backend stuff and the reason py torch is so fast is because you're getting it to C really fast um okay so why does this not learn do you want to like look at the gradients can we do that is that GNA make any sense py torch is 20x faster that how does this not learn this should be the easiest problem there should only be [Music] gradient arel is the most frustrating thing in the world there's no Nan in the gradient it just like doesn't really work okay ready the most dead simple thing it's called vanilla policy gradient okay we're going to write the stupidest thing and we're going to see if we can solve it I will notice there's no way this could ever solve hard mode because I don't keep the stuff together okay and make one model called actor actor is a very simple model all it does is be an actor we call the actor sometime we do a training step and inside this training step we run the model the model is an actor we optimize the model using atom rename this frustration Chronicles ew torch we don't pass in the Old Log distribution anymore cuz that's stupid and this model only returns a log distribution we use the vanilla policy gradient which doesn't even use Advantage all it uses is reward we only have one loss it's called the action loss and here we'll compute the backwards pass on the action loss and then we'll run the step and then we have the action loss we realize that and that's great okay we also have a function called get action that's identical except instead of doing that we just do that okay now um this is all probably good we can copy this we don't have value any anymore because evaluation is dumb we have time uh we always need T range and we'll run it for appox okay as you can see this has a lot less stupid uh discount Factor discount factors are lame we don't even really need reward to go we could just use Rewards keep next Replay buffer that's fine train steps we can only do one because it's vpg batch size can do 64 okay uh yeah that is dumb don't need that stupid all we have is the action loss don't have the entropy loss don't have the critic loss okay it doesn't learn um let's go over here and I'm sure they have an implementation of vanilla policy gradient no do have po though po solves it perfectly and almost instantly no never mind right it does solve it with the on policy but it takes a lot more time there but whatever I don't care about that um wait that's size two no size eight okay fine yeah their po solves it takes a long ass time but solves it okay uh wish there was a way to pass in parameters to this let's try size two okay this doesn't seem to work at all sometime it gets lucky and gets reward most times it does not uh are we calling train step yeah we call train steps we don't need that we don't need that samples action loss [Music] great so I'm really using a soft Max the replay button oh I'm not using selected action okay something's wrong yeah so this is actually just wrong you can't no I can't do that that's fine but uh need a mask based on selected action and then we can call this mask reward multiply that by just mask times reward and we can put reward there I mean it looks better no okay okay vpg looks to reliably solve this with if we get three in a row we consider it reliably solved okay three in a row great this reliably solves this with these oh no but that's size two okay let's go to size eight let's see if it solves it nope size eight doesn't solve it okay uh we don't even need layers here I know we don't need layers so we can just do that we don't want to need that we can just do that hidden State can be tiny because it's literally a pass through net if this isn't working something's just broken in the way that we probably have bugs in our where we're doing soft Maxes and exps and shit uh uh does it solve it with size two okay it solves it with size two but when we go to size three it doesn't solve it anymore good this is like a pure this is like a pure implementation of trash filth and garbage okay that one's all done uh but three is unreliable okay so we're seeing the same behavior with wherever the bug is it has to also be in the uh this vanol policy gradient okay no more critics no nothing all we literally have is a dumbass actor um that's how stupid this is and it doesn't work this model should be simple enough we can look at the gradients actually I think we can even go simpler here let's literally do this they should solve it linear layer should solve it right nothing good I think we can actually look at the gradients of this one I think we're getting to the point where it's simple enough uh I wish there was a way don't need gy make okay uh the linear layer is of course unreliable how the hell is it unreliable this should learn the identity Matrix I think this should learn the identity Matrix okay uh print model. L1 weightp all right 8 by8 identity Matrix what oh that's with size equals [Music] 2 it's not even solving it with = 2 it kind of solves it uh do a linear layer we do like tensor okay I mean you see why it gets no reward right like this is just junk um why is that always zero okay sometimes it's not zero it's two oh we can also get rid of reward to go we don't really need that [Music] why is that sometime five and three in shit huh I don't really understand that but whatever oh that's cuz it's it's doing uh it was doing a where to go but okay so those look right masked reward the batch size is also crazy we should be able to do a batch size like four I I want you to learn the identity Matrix please [Music] it got reward that one it didn't get reward like everything should just be pushing it perfectly towards the gradient we can get rid we don't need adom let's use SGD I I'm thinking it has to be some of these log soft Max and Xs okay Master W looks correct let's look at log this.x let's look at this and knit with the identity Matrix so we can try that uh well we also don't need a bias wait a second why does that not have [Music] reward so identity Matrix actually isn't the perfect solution but I also don't understand why this doesn't have reward oh I think I might have just broken the reward thing yeah I broke the reward thing okay that's a different bug let's put reward to go back in it doesn't even matter it's not like it hset okay the identity Matrix is actually not the perfect solution because it's not downsampling the others enough um if we really want to form the identity Matrix that works I it's like multiply 100 and subtract 50 should be pretty extreme okay now it gets the reward every time and it has pretty low loss which is good great let's go back here pump that um nothing seems to learn now all right let's look at some gradients [Music] [Music] I hope we like get something satisfying out of this okay these gradients look what's backwards but that's actually right because we're subtracting the gradient wait everything looks like correct back to doing this just so we can see it I don't know I should be able to make the learning rate one okay that works do it work if I don't okay we got a solution we made the learning rate one me there just shouldn't be any noise in this problem okay all right all right all right now let's try with a slightly larger size four okay it solves it by the way how satisfying is this we can like see the gradients and shit see what it becomes more like an identity Matrix every time okay wait a second why does this gradient look like this oh cuz we had multiple we had multiple steps that's okay and they all look correct you could also increase the bat size it doesn't matter we don't need stable gradients there should be there should be zero instability we can actually make the bat size one there should be zero instability in this problem I guess just sometimes it doesn't have any gradient this is fire Bros does everyone understand what's happening so this is the gradient and this is the single weight Matrix um we have a reward to go that's all fine all right let's add some things back in until we break it well first let's take the oh well can we go to Adam first matter if use Adam or SGD I don't know how Adam I don't know what the update will adomus oh it's like instant everything good about SGD even better without um how about a learning rate of one2 is that going to work the learning rate is just too small like it doesn't learn just because the learning rate's too little even though this looks better than that Suess it has a slightly higher probability no let's use this okay I like the learning rate is just too small it's getting a reward of 10 why is that not get rid of that see print okay it learns it's just slow even with Adam I don't know well I mean this is useless for fixing anything else like it doesn't learn okay let's Jack The Learning right out still doesn't learn okay well that's exciting at least well I don't know if the jacked up learning rate works okay if I'm doing uh all the fancy stuff I do in this one okay let's go back to vpg because that's all it no we have no action loss okay interesting all right all right maybe we are getting somewhere with this so we have a reliable solution from Tiny grad vpg uh as long as we set the learning rate to one1 and then I mean we should be able to set the batch size crazy high I don't think it matters if anything El want after okay good oh that's so beautiful I love watching deep learning happen beautiful okay uh this one doesn't work why doesn't this one work even with the big is it the value function we can get rid of that too call this master Advantage advantage and reward are kind of the same no not L um is it because our actor is very complicated we're getting rid of the second layer again I knew the second layer was a mistake doesn't work okay we're using the same advantage using the same action loss um get rid of all these other losses oh it's going to bitch if I do that though I don't know I just multiply by something really small no nothing let's go to a simpler actor let's use the same action function we use here we can actually watch gradients again should just work right oh that solves it that solves it but if I put two two layers in the net no luck can't learn the identity from two layers is that stupid if I switch it to this one hey Lawrence but if I do this it doesn't learn t h this is something like deep learning doesn't work shit single layer all right put the gradients we put the gradients put size back down to four so it fits on one wait why are those gradients so small I don't look small anymore the first one why do they start so small like that let put just numeric error if I just didn't get any reward there does look right also what is the default wait what no I told you not to put that set print op so that did nothing when I wrote that there aren't paying attention why is that gradient start small no maybe okay again maybe it's just not it gets no reward it's not sufficiently random in the first but then it gets reward and it starts to fill those in and I'm more happy but we do this and it breaks no gradient why is there no gradient um let's multiply that why is there no gradient for so many of these I see it get rewards sometimes is there like a reu death problem no there's no gradient here either we got a gradient for a tiny bit it goes away we don't need a hidden State size to be big learning rate too high we do have a high learning rate so tiny the littlest bitty gradient I've ever seen wait what we just got lucky with that one tiny gradient tiny gradient okay this maybe like a log softmax problem again this this might be our problem debug is not going to help where's the gradient going still very [Music] small cuz that's also being multiplied by 100 too H why is that the why is that the output of the model just just collapsing to garbage fast maybe the learning rate is too high okay at least now we have some gradient okay it solved it kind of yeah okay all right you can't have very large gradients with uh if you make your learning rate too high your model will become crap we set our learning rate high the distribution converges to a single thing very quickly don't really understand how that is but whatever could it be doing n of the weights yeah maybe I mean whatever it is the same problems in torch I feel like they have some trick for this I feel like I saw some garbage about this in a2c for sort of learn sometime that should help with the gradient too with the entropy loss t ball should always give it a gradient I think I mean okay it makes sense if we Jack the entropy loss up that we're going to get that behavior all the time lower the ENT we don't see that we see a distribution that converted to something why is that one still B it's like this one started to learn that that's good but it hasn't learned anything about what to press in that case there TR have jell you yeah we can just jell you try jell here it's like kind of solving it those look pretty good I don't know why is that one still that though it's actually it's always that one I don't know if we're just getting unlucky but it sure does seem like the second action here always has that no that time it learned and that time it was that one that didn't learn look at that like all the other ones learn but that one doesn't learn what sense does that make it should learn pretty fast because it should learn to pick that action in that state yeah look how some of them just are not like they're not learning TR jelly you look better okay R you death all right I like the hypothesis interesting see seems a lot more reliable now there you go look we're at 10 all right dying rues eh where's your prize I make your VIP congratulations [Music] turn the biases back on cool boner P you'll get another timeout you didn't suggest reu death you were just shit talk talking you're a coherent suggestion for you and even if you did you know about the boy who cries wolf it doesn't even matter if they're actually ever was a wolf because he cried it too many times and you know you can't just keep listening to boys who cry wolf that's right okay that's fine and the Jet back on there okay reward of 10 let's try Advantage like it works it's just flakier we need with PPL okay Advantage Plus po worked Once flicky garbage I jack the batch SI back up 10 not 10 go back to our p torch version it's fast switch this to jelio torch have jelu alue what does torch have I you like torch NN crap torch and and crap l l is a good loss function L use a good activation not as good as jell jell you was the winner uh can also just try that just cuz I saw it in it's another stupid hyper parameter just CU you didn't put the hyper parameter oh no wonder this doesn't work I left this in there if anything that made things [Music] better I saw they had a value scale of that so oh unreliable garbage check the hidden State size back up I don't [Music] know well not using the reward function not using the value function seems to help okay now is there a bug in the value function 10 10 as soon as we use the value function I'm too slow let's look at these values shouldn't do anything actually that should change nothing why does it change something that time it got lucky and solved it no solve put the batch size back down so we can see some normal size prints why is it learning negative values of states okay this is learning all the values of states are good I mean that's sensible okay fine it eventually learns that all the values of states are good we have reward of 10 but if I put this in here which is just the reward minus the value do I use this Advantage I use okay I use it down there I wonder if there's some bug is it actually the same if I do mean square error really doesn't matter which way I put it right looks to be the same level of crappy it's true that in this problem the value function just adds noise I batch rtgs minus v. detach okay I don't know we can we can add this let's add this we doing my critic loss right we'll just leave that down there and let's add in this this normalizer since they seem to have it I wonder what that does no so don't need to detach there anymore shouldn't matter but okay still socks and his flaky this mask can go up here this has nothing to do with we're g to call we're going to call it action mask uh log distance not associated with the value oh I don't care about that well that's kind of ridiculous um what do I actually want there oh probably is the best place to get that I mean it's the number of actions but let's just take a quick look at the action mask and make sure it looks reasonable what does that probably should be Boolean think that breaks anything but whatever doesn't hurt solved hello hello hi we're streaming hi T it's Alex she says hi Alex do I want to go walk and get lunch yeah you want to you want to get a you want to go to the sub place yes bro what's it called Caesar shit wait yeah yeah don't tell them man some weird might show up and we're going to knife him man is that going to get me banned from twitch I've been trying to get banned from twitch it turns out they're ripping me off they're only giving I don't know why I can't hear you oh it's cu the internet sucks I would love a sandwich uh well how about we go in about 30 minutes okay perfect that sounds nice I see you soon oh I'm going to knife him man is that a threat of violence is that good is that going to get me banned from twitch [Music] uh knife the sandwich no I'm referring to the person who I am going to stab with a knife no clips you guys know there's no clips see taken out of context it looks like I'm threatening somebody with a knife and I am but without seeing the other context you don't understand all right so we have 30 minutes maybe we'll stream after lunch too do we have a lot of people watching this shit no 112 all right you all like watching me fail at reinforcement learning this is apparently a popular topic RL sucks and's dumb I might be banned from twitch when I get back though and if I am you can find me on rumble or whatever that other one was the gambling one where they where they pay a fair do you think I trust that company um they tossed who live oh she got a three on vacation uh no no no I don't know if we kick for that that's kind of interesting what are we kicking um oh kick oh kick the the fake twitch wow it's been so long it's been so long we've been failing at reinforcement learning for hours this is terrible okay so like all we kind of do there is we normalize [Music] stuff why is the entropy loss so big let's put the J back on why does this not work no I want to clip the rewards like that okay I mean to be fair this value function is all just noise in this problem because all states have the same value can we solve cart pole let's see if we can solve carple okay we soled carple sort of went a little unstable afterward but see you can see the reward here does everyone like see this this is like way too small for everybody lunar lander we shouldn't call them EPO we should say episodes okay we're getting we're sometime getting good rewards it looks like do you want to get the plotting code from yesterday so we can plot some stuff like had a knife somebody the plotting code was cool uh let's go find it o that was the best one yet any conclusions on decision Transformer we just remembered that reinforcement learning doesn't work at all [Music] what okay I should really this is the kind of stuff that I should really be using uh some GPT shit for wow I didn't know the critic loss just went to the moon let's stop plotting that one also can I do pause zero I don't actually need to pause do I slow enough why would I make it slower pause zero is not what you want oneus 6 that's not real time all right the entropy loss goes up the action loss stays the same but that's just cuz we're like normalizing shit and it doesn't have anything to do with anything uh the rewards seem to stay the same because nothing actually works it is wild how high that critic loss is we should try a more aggressive discount wait we're getting positive rewards sometimes this might be working was positive rewards that's stable it's a pretty aggressive learning right still can we solve this before lunch may we should have a learning rate schedule sounds boring oh oh look at that graph go up surprising thing is that it works at all yeah I agree it did one Perfect Landing and it'll never do it again yeah for all the people who say AI is here I've never tried RL it's so true whoa brosies yo I think if we just let it go let's just go is that the best one you've seen yet I don't know let's let's randomly change a bunch of hyper parameters and not do a controlled experiment at all yo look at that reward go up boys ready to normalize the advantages that's right there's some like woke thing about that I think you you got to normalize everyone's advantages we need diversity and and we need to normalize advantages that's woke people are on to something guys you know let's not discount them all right so this is learning maybe we should try their magic learning rate from the other paper from the other implementation to I wish this was faster look look at the the thing going up yeah yeah up the direction we want to go let's go there's nothing I love watching numbers you know I love watching numbers we didn't get decision Transformers to work and that's kind of sad' be like nice if we did you know up is not discriminatory up is the direction we are going up is the direction yeah that's right you remember that remember that eade commercial back in the day with the monkeys and they turn the chart I think we're going to land on the moon I think it's going to work no no that's down it's not the direction we want to go go up number go up let's go right it's getting slower probably because the episodes are taking longer I should plot that too plot episode length because we got to go get sandwiches soon Alex is upset when I make her wait we only get it's one it's one shot it's one shot when it's that's it's on the final we don't talk about Dogecoin no it's slower I don't think it's the graph I mean it could be the graph uh but my theory is that the episode so longer but actually no it's probably the graph it's probably the graph the critic losses low okay one shot come on land on the Moon where is it oh there we are okay well kind of sucked um Loy boy all right let's let's mess with hyperparameters randomly and try again no that's the problem it's floating that's why it's taking so long now where's my graph so just not happen for I and do I have I reuse ey somewhere don't I don't reuse ey train steps and you'll be a lot faster now I think that a lot of the slowness was just my stupid graphs uh oh see look we're getting some long episodes now rewards going down what hyper parameters did I randomly change last time I don't remember socks oh it's way faster now at least wow how slow was drawing that graph it really was just drawing the graph every time that was low okay sometimes we're getting these very long episodes I say but I guess okay sometimes it's long episodes that make it slow yeah we we we've landed in we've landed in take forever land see look it's now it's now maxed out the the time and it just floats around we can watch it float around sometime it just must get massive rewards from floating land land land land land land land we actually saw the same behavior with the uh the other example okay uh I mean no no no we're not going to start we're not going to start reward hacking uh what might help with that I can change the discount Factor let's do controlled experiments uh there was something else I wanted to change more that was more meta uh but I don't remember what it was okay we now have we now have a discount factor of uh 99 sometime it's slow because it takes the whole episode well I guess the other problem is there is a sampling bias towards times when it takes forever and more of those are going to start showing up in the data set oh maybe we just increase the size of the replay buffer I think that's definitely going to help okay do we like the new discount factor I think we don't that's sometime that's going off the rails put that back and let's increase the replay B for to 20,000 yeah I mean there's definitely a bias towards sampling the episodes where it hovered because those are much longer episodes remember also the policy that we wrote out at the end is the argmax policy uh we don't sample at the end yeah back to hovering all right let's try a lower discount factor and a larger replay [Music] buffer oh I also want to change a figure size that's what I wanted to change yeah I don't know how to do [Music] that someone wants to post about how to do that I will change the figure size to make the figure bigger for all of you oh figure it takes a parameter crater no okay not a good discount factor I don't know we'll try that I don't like the replay buffer just make it that a little more normalization oh wow wow that's a big big size now oh that's going to take forever to draw I don't want to L at all now maybe maybe okay now we're doing slow episodes reducing samples from it we could do that I don't know I don't want to start how many hyper parameters are we going to introduce to make this thing work great worse than ever the best discount Factor we saw was this maybe we need a smaller replay buffer [Music] um Le it didn't flow yeah I mean sure I think that's going to help but who knows oh I don't like how large that is but I think the conclusion we made is there were bugs but like there weren't any major bugs well that PO one was actually super major and it didn't seem to affect anything when I fixed it so I don't know about that also change my critic loss to it's getting some real positive rewards now oh oh oh oh come on that was so close what did I change I don't even remember did I change my fig size even that's yeah it's 1010 so that's good the smaller replay buffer makes it more on policy which seems to work pretty well we can try the Kathy bless learning rate we can also try just letting it train longer and maybe it'll figure it out these episodes get long and that's why it could slow we have to go get a sandwich man land on the moon bro you got this moon for you you got [Music] this all right more train steps lower learning rate few more episodes the entropy loss I think like the more that goes up the better we're doing as long as that's low the thing's just pretty much acting randomly that's a way to know I guess when you've converted Ed so the orange here is episode length the uh blue is the return the this is the critic loss uh this is the action loss which yeah I think there's a reason it's always zero because the advantage should try to get as close to zero as possible makes sense if I disable Advantage okay the critic loss is getting low so we're starting to actually learn a model of this thing learn which states are good which states are bad oh feels like we have to train it longer still maybe there's not enough of a penalty for hovering this is getting stupid though again my point is that this doesn't work I'm sure I can find some set of hyperparameters if I sit here for another few hours that will work but something just seems so broken about this whole thing and that's why RL is dumb and doesn't work you feel like track Mania would be easier than lunar lander it like might be I mean there's also some really counterintuitive thing in deep learning where bigger things learn better um that's not that counterintuitive like one of the explanations you get for it is that if you have a truly multi-dimensional space uh local Minima become less of a problem because given a random point the odds that you're in a local Minima are you know one over two to the number of Dimensions you have assuming each Minima can equally go up or down um so you're like oh but you're using an Optimizer you're more likely to find them yeah but if the number of Dimensions is a million you think your Optimizer is going to find a one two to you know one over two to the million thing probably not um like optimizers aren't that good uh so this is using actor critic is this is another one of these like it's unstable in the same way Gans are unstable you have you have two competing models I don't know if they're as competing as Gans because they don't think you back prop through the critic we are detaching the value function uh the other thing is that we just may not be training this stuff for nearly long enough tiny grad's way slower than torch on this um again tiny grid slower enough because tiny grid is usually fter like on a big net tiny gr is is faster than torch on Mac but on small Nets torch is just going right to C and that's just very fast tiny gr is slower for small Nets um yeah we want to see that entropy loss going uh going up uh the entropy loss is just a loss function on the diversity of actions uh it it forces it to learn more robust policies um I don't know like sometime it gets good episodes now and sometime it doesn't uh T guards faster on large Nets no just the kernels are better on M1 anyway um the small Nets is just is just so much overhead in in launching kernels and stuff I think and torch is just using CPU which is even faster because there's no overhead just function call yeah um I don't know and the other thing is we just may not be training yet for nearly long enough these things just train forever we definitely have it learning something the numbers are going down the the Returns on the episode are going that way and that is the really fundamental one um we have some good Landings in there I think what we'll do next is we'll just uh every hundred we'll have it render to the human and we will watch one good landing and then that'll be the end of the stream and someone someday is going to make reinforcement learning good but I don't think it's today NOP okay every 50 I shouldn't have done 50 because that's the same one as the graph I don't want that 51 okay so here's where we are right now crash there's some good ones in there I'm pretty sure I don't know I like I I wish I was just missing something fundamental but I don't think I am I think it's just just just bullshit I I we fix that bug in po I'll commit that bug fix uh for anything for all I know the bug fixes end up making things worse the house is a mess still streaming we're watching the lunar lander try to land oh oh Bros yeah know we're going to go get sandwich after after it lands after a successful Landing no that was not in between the flags are you happy that it's sunny outside we missed the walk hunger strike until RL works if only nature works that way okay has it learned now if it hangs out it gets more more reward it's a pretty good [Music] Landing I mean it's clearly learning something I don't know there has to be one good one in the next oh this could be it come on no no move over move over move over oh oh all right remember yesterday when I said AGI was close send this send this video to a Doomer okay send send this send this video to a Doomer and ask them if they can get the lunar lander to land no no no no we're not going to be able to eat till it lands in the middle of the flags if it doesn't land in the middle of the flags this episode we're just going to have to try it again because that's the real Spirit of RL just trying random seeds until you get one that happens to be good is this easy with classic trajectory optimization yes but you again you missed the whole point it's also easy with a joystick you don't get to experience the joy of reinforcement learning if you just do that like I see some rewards and they look very good oh come on this is it oh yeah right in the flags oh Bros bros Bros all right that's it that's it I it just it landed they good enough if if we if we sit around here and we wait for something better we're GNA we're going to have a going to have a sad time wait this one might be better no this one's not going to be better move over oh [Laughter] Bros um what's also interesting is the that the AR Max policy doesn't work this is this is now switched to the AR Max po which just seems to do this enjoy it Bros let me see if Alex is actually ready to go you ready to go a sandwich are you actually ready to go a sandwich all right so I think the lesson that we learned here and we're going to we're going to let we're going to let subscribers non subscribers talk I think the lesson that we learned here here is that uh when you're doing RL you have to lower your expectations if if you think that you're going to make it work with the fancy decision Transformers algorithm your expectations are too high uh if you think that you are going to make it work reliably your expectations are too high if you think that sometime it might get slightly better at a task after you pour an absurd ount of compute into it that is the right expectation for RL um we're not going to start like like yes with adding things to the loss improve things yes but that defeats the whole purpose right why not just feature engineer at that point um reward shaping is just feature engineering uh yeah you just you just need to you know lower your expectations um and that's what I need to do for twitch because I don't think they're going to contact me I don't think they're going to going to apologize to me for wrongfully Banning My Stream uh I I think that that that I deserved what I get and you know you just that's just life this is not this is just dumb you know just like garl how about searching hyper prims great now we're searching hyper oh yeah I'm sure I'm sure we could put a big grid search sh those hyper prams um and find ones that kind of work what was the learning rate in the in the example 4 e minus 7 or 7 e minus 4 yeah how did the hyper pram search go for that right oh is this the best landing yet yeah oh oh oh so good so good except move over it's pretty good no no no no Kathy is 3 e minus 4 the the one in the uh in the other one was 70 minus 4 okay so it learns it learns somewhat reliably is it great at the task not really do you not understand why it's not great at the task you have no idea we looked the gradients it was 74 yeah I remember um I remember trying stable baselines once and like it just couldn't solve lunar lander with the stock things and I complained about it this is why we don't let nonsubscribers talk uh but this was actually a game you could play I didn't know you got to do RL to find the best way to do RL no someday decision Transformers I actually have a lot of Hope for um and oh by the way do you know who made upside down RL I was I was reading the papers last night upside down RL was invented by one Jurgen Schmid Hooper uh he still got it Schmid hoer still got [Music] it I'm send this video to a Doomer remind them that AI Doom but no it is true that once we get good reliable solutions to lunar lander we are just one step away from good reliable solutions to everything and that seems to be how these methods work because they scale really well yeah he really liked turning the letters of RL upside down and he wrote this a ton of times in the abstract um what we go you go you're going to go without me we're not saving checkpoints between runs okay the last run of the lunar lander do you want do you want to watch it try to land talking do you want to see what we did sure look look look we we made that Oh I thought you were talking about a real lunar Lantern no no no look look look look look going to kind of work it's going to kind of kind of okay lower your expectations it's not really going to work oh and that's the end of the episode it just stops right there all right guys thank you for watching today's stream uh these two streams should be put in one stream and the whole label should just be RL is dumb and stupid and doesn't work great thank you byebye don't kill anybody I was kidding we're not really going to knife anybody that was just to try to bait the twitch people into Banning me what what I'm G to go I'll in are you that itching for sandwich I want coffee you can have that coffee you want that coffee I want to go to California are you're going to go to California Bean you going to make conversation with people huh say goodbye you can say you can say she's just itching to go I I I don't know I don't know what to say goodbye everybody