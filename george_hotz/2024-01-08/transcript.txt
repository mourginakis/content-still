I forgot to unmute uh I heard some devastating news I was scrolling Twitter and I heard that yesterday's stream uh was not recorded uh I I uh first off did not delete yesterday's stream uh I believe due due to the due to the the the incident uh that occurred on stream that uh twitch did not archive that stream due to the very serious uh incident um so unfortunately that stream will be the Lost stream uh the cool thing thing about the Lost stream is that only the uh people who were here know about the incident and know what happened um but I would like to I would like to uh deny any responsibility for deleting uh yesterday's stream uh which I vehemently did not do uh but the stream was was lost uh thank you to uh to to to Twitch uh unfortunately the the incident now if if you in chat know what the incident uh was uh we don't have to discuss the incident uh we don't have to tell anybody else what happened you have special knowledge about what happened on January 6th uh the January 6th incident that caused the stream uh to be lost to history history wow we have some lore seriously don't tell them seriously don't tell the noobs man don't tell the noobs okay all right the FBI would actually uh be interested perhaps in investigating this incident um but yes there was an incident um on on yesterday's uh stream uh if you actually have a backup copy then that might end up on YouTube um but also you think I have a copy you think I record this shit you think I have a little folder where I carefully put together geohot streams uh I don't uh would I work for the FBI you're a subscriber asking that question all right we're going to get some coffee and then we're going to go through the theory of why RL doesn't work uh so we're separate from the incident that occurred on yesterday yesterday's stream yesterday's stream was mostly a struggle session with decision Transformers uh and I think we're going to go over a little theory about why RL uh is dumb and doesn't work for those of you that don't know it and we're going to learn some stuff no I don't think no it's George hot's archive that doesn't have the incident uh I I just saw this I just saw this on his Twitter uh so the incident will be lost to history um I of course uh have no problem with with with the incident becoming public but I I I do also think that whatever people imagine who were not here yesterday who know what the incident was that whatever other people imagine is way uh way better than than what the actual incident is so uh we can we can we can leave it at that did I talk about it earlier I sure hope I didn't talk about it earlier shit man all right let's get some coffee and those the what frame youing history does it have a foot no fucking way Alex go is the footb lower than no there's no way get I will not sleep on a bed with a footboard all right all right all right well that that I want to see measurements this is St jumptown cold brew what incident you're not to know you don't get to know about the incident if you were not here during the incident you don't get to know it's one button on OBS to record yes I'm aware but then I have to carefully put the files somewhere oh what you think I'm made of files I mean as proof live is way better I know come join me live soon we're going to be moving to kick because twitch uh I saw a tweet that yeah the files they're in the disc man um I saw files I saw a tweet that said twitch only shares 50% of the revenue they make off you with you and Twitter shares 97% and we just can't stand for this so we're going to have to move off twitch this may very well be my last stream stream on Twitch um does it make way less I mean it's you guys paying you know Ty out kick look look let me tell you about right-wing YouTube it won't succeed uh right-wing twitch will also not succeed um we could eventually move to YouTube that's more uh plausible what does YouTube Share I imagine YouTu has subscribers but you know we're gonna give we're gonna give twitch a chance we're GNA give twitch a chance until next Saturday to rectify this situation and to offer me at least a 7030 split um and if twitch doesn't do that well you know this might be my last stream on Twitch uh donate 10K to your college fund no um it's not about the money it's about the principle it's about standing up for the little guy so let's talk about why RL is difficult so in normal models you have uh data X and you have a model and you have data Y and then there's like some function oh this is terrible um there's some function of model and you have data X going through the model going through data y we back C through that and everything's good but in RL unfortunately this doesn't stop like this so your your data y determines your next data X and so on and so forth It's not really written that well let's see if we can find a better explanation of it uh in the uh but the big problem is data is non-stationary um so non-stationary is just a f fcy word of talking about how it changes over time like the distribution changes over time so you don't necessarily need the model to affect the data in order for it to be non-stationary for example the most common example of non-stationary data is the stock market uh the stock market does not have the same function generating it over time much simpler function was generating in the past but over time that function changed so the data is non-stationary changes distribution uh and in RL it's even worse the data is uh dependent on your model what is all this crap why is it lighting up like that don't do that so let's take a look in the r book and see this is the famous RL book uh Sutton is this guy's like the god of RL uh we love this guy too look at his look at his look at his political beliefs downsize DC why you are a Libertarian uh he also he talked about oh man no Rich Sutton's great uh so if you guys don't know Rich Sutton so let's see if this this book goes into why RL is dumb and doesn't work I would imagine it doesn't because it mostly shields for RL um how to map situations to action so as to maximize a numerical reward signal oh that's even worse oh there's there's wait it gets even worse than this oh man credit assignment okay like now I'm seeing why RL doesn't work at all because it has basically every bad thing you could imagine about data like H like I wonder how explicitly this stuff is T tested yeah you you can test you can test each one of these things uh independently right imagine an mest but the mnist has one pixel that like swaps the nine and the six or something um and then you don't see any of the ones with this one pixel until much later on well so yes I I agree with you that they're doing it with quadrip heads and RL um if you haven't seen the Deep Mind soccer robots they they look very impressive told only that the object was to score the self-learning programs spent about two weeks testing different moves it disgarded those that didn't work built on those that did and created All Stars there's another goal and so they definitely look cool um I think the problem I I mean we we may just Brute Force our way through all this stuff so sample inefficient uh but now like you get to all these reasons why it's brittle think about just just focus on that one to start with um the the uh I talked with a bunch of guys at neuralink and they talked about how annoying that this this was in uh in the monkeys because the monkeys over time learn different things uh so their old data their old training data no longer works on the current monkey because the monkey's changed um well let's talk about okay so something interesting to note is that search always works you can always do search um imagine doing search through an entire chess game imagine doing Minimax all the way from whatever state you're in to the you just don't have the compute well this gets could you guys post I I remember that link from before about debugging RL I think there's also like a record now there's like a fast Atari paper here debugging let's see if it talks about these problems [Music] what's beta in po do I have that oh the strength of the entropy regularization I mean we we we we definitely have that by the way I should commit those fixes to to Tiny grad but we don't have to do that now this is a Theory stream so this is our this is what they're calling beta this this entropy scale okay we have buffer size interesting we should have just read this this is this is pretty useful is the number of pass passes through the experience buffer during gradient descent yeah we have this one it's called train steps that's interesting we don't have this we just do an episode each time interesting all right let's let's should we do a little do a little clean up of this that's extra [Music] extra that's junk that's junk we'll keep light up button as a great implementation of light up button we're just cleaning up don't let this get out of hand uh delete that [Music] stuff rewards episode Lanes delete that stuff that stuff can go that's changed doesn't matter that's all junk this stuff's junk um we can put the detach back here no we won't leave those comments I kind of like them that can go there we'll leave jelu jelu works well so let's copy and paste this in here and let's adjust to make sure things work well okay so batch size used to be 256 we'll keep that uh replay buffer 20 48 uh hidden units put that back to three four it's probably a good learning rate do we need a value scale that one was kind of always a stupid parameter train steps what do they say to use oh they call this number of epo I like train steps better combined experience replay take a look at that in a minute let's see if we solve uh oh also 5,000 is crazy we eventually saw press the light up button just takes a long ass time I did kind of like the graph in there want to put it back okay I at solve now it's just really uh slow that should make it a lot faster all right solves that really fast let's go back to episodes 40 great solves it super fast let's see see if this also works on cart pole get rid of that no broken on Card Ball damn it all this work and it made card pole worse see which ones we changed okay hidden State size was 32 uh Mac replay buffer was 30 was 2,000 learning rate was episodes was [Music] 40 go oh discount factor is 0.99 maybe that matters no don't we had good progress there with this worked oh okay don't normalize the advantage stupid uh what is the me of the advantage big until it's not get rid of that get rid of that get rid of that that back here put this back here okay we're back to solving cart Paul at least well I don't know why I did that that time now we've had card Poole running for a while so this is in tiny gr it's called beautiful card pole and we're just changing it a little to make it more beautiful um J seems good everywhere you can choose environments uh is the time fast yeah the time's fast it's about five seconds okay we converted that we fixed a bug there that was a serious bug that was the most serious bug we found this morning um they don't call it entropy scale they call it beta I like entropy scale better 20 48 just so we're in the typical range there PPO Epsilon is in the range hidden units is in the range instead of hidden State size we'll call it hidden units let's put them in order spell that one out so it matches the others car batch size the entropy scale we have the uh play buffer size uh Epsilon hidden units learning rate number of epoch I like our name better train steps uh we don't have anything like time Horizon oh after that should be learning rate number of VX time Horizon and then this is Max steps but we call it episodes and then we have a discount Factor should we call it gamma I like discount Factor better don't use Greek if you don't have to okay it seems less reliable than [Music] ever fix bugs and oral gets worse okay I promised this was going to be a Theory stream I don't know go back to in tan H like it was there's no reason to change that seems better not that much better whatever um Can this solve light up button okay good Sol's light up button can it solve lunar lander not even close is it just because we didn't give it enough episodes okay whoa whoa who we're going off the rails again here we're just sitting around trying bullshit why is the entropy loss not more I'm going to add more what uh open pile does not use any RL because RL doesn't work add another digit to these losses and critic loss can have an extra let's also change that thing in the jet to not be an assert but to be a warning okay take one last look over these changes let's commit them to master this is just committing the work we did this morning uh so what type of ml is used for open pilot supervised learning um yeah we we've experimented quite a bit with RL and had no Real Results the other problem with self-driving cars and RL I hope that one doesn't cause a problem I don't have any checks to make sure that is Sears out right do I have like a test jet to like test that yeah pilot religious guys in India uh all right let's let that test we finish so what we were going to Google was combined experience rep play see what this is all experience larger buff made learning slower this is old we have one failure in testy type Al but I bet you that's a known issue that just doesn't hit on CI for some reason let's give on to go over this okay some environment choices we made all the things explicit we moved action mask up to there renamed that fixed the bug and po broke that out renamed that to episode number moved those up to hyper [Music] parameters uh move those parameters renamed that [Music] parameter added a detach there and yeah that's all looks right okay none of those are problems I don't know what that one is okay some fast ma stuff and some metal uh if someone wants to play with this if someone wants to go do do search on these hyper prams and find ones that are better and find ones I can solve lunar lander I mean I'm confident that the whole thing can uh you can also put back in that the action regularization if you want kind like a batch Norm on the action I bet you that's only good on some problems though depending on how the rewards look I'd also like to see a whole set of buttons that look like this so where was that other where was that other uh debugging RL [Music] do not try stuff and run to see if it works also I kind of feel like this hasn't gotten any better I feel like this has sucked equally as much when I was trying this three years [Music] ago um yeah these these tips are pretty Universal and that they're mostly the ones that we did what's the latest stuff on this does does anyone have any stuff that's happened in the last two years um also I don't know how much you guys agree with the hypothesis that ml is just going dark like all this stuff now exists in companies and companies just don't share any of the secrets um which would just suck like if that's true uh I don't think it's a smart strategy um but I mean you look at you look at open AI like not talking about the mixture model and I mean Mixel didn't find out that open AI was using a mixture model for me everyone kind of knew that I'm just the guy says it uh so that would suck but it's also just possible that this hasn't gotten better well the corporate greed is real that's not that's not why it's happening it's not corporate greed it's it's it's it's a misguided belief that somehow your moat is how you your your two weird tricks for RL it's not that's never your mode as a company and and if you're by the way I don't mean to be flipping about how comma train stuff I will we've explained the whole thing in the blog post it's just there's not like a oneline way uh that I can explain it to you uh we use a simulator but we use the human policy um as the ground truth so we will it's called a small offset simulator will shift the simulation a small amount from reality uh no I don't think the mode is compute either I I don't think it has anything to do with compute or your tricks the the Moe of your company is your ability to execute always um if if somehow that's not the mo of your company like you can get other Moes there there are modes like regulatory modes there's modes like modes of just making the switching cost really high but your mode is never some some ml trick you have I don't think the is compute either I think anyone can buy compute your mode is the fact that you're the company who can actually no I don't know if it's being closed Source or there just hasn't really been any progress in this stuff because look I mean this this Reddit post is from 5 years ago and it's all still good advice make sure I didn't break tiny grad that look good H tomorrow at work I'm going to go through and start deleting uh deleting a lot of stuff interesting I doubt that was wrong uh what was the Reddit post about it's about um uh private data is also a decent mode I wouldn't start sharing your data as a company but a lot of times it's not the data source itself it's your ability to cleanly curate the data um this is one of the things that I consider a mode at comma again our mode is not the raw driving videos you can go out and you can gather uh bajillions of driving videos for relatively nothing multiple companies have um our mode at comma is that we have a data we have an extraction pipeline uh actually using and cleaning up those videos to the point that you can put them inl algorithms is not that easy and easy to write bugs uh so it's the kind of thing that you iterate on in a long time and you can build longlasting value there and again there aren't tricks it's just good um so I do think that that is that that is one of opening eyes modes and I think that that's a reasonable mode to have as a company again don't don't share your data um but to not share whatever tricks you're using for training you're in a worse you you might think that oh well I'm going to keep my tricks secet um but you're going to end up losing if if you're in a company and you do this because if you keep your tricks secret and the world doesn't start using them the world is going to move forward and the world is going to use other tricks if you genuinely have tricks that are good and they're going to develop tricks that are maybe incompatible with your tricks and it ends up being more work for your company um so a lot of times open source is the reasonable move even if you're a greedy bastard um because of well think about think about it just from the perspective of a fork if you make a fork of code you then have to maintain the fork of code um but if you get it upstreamed then you don't have to maintain it the Upstream people will maintain it uh so very very often you want to get your stuff upstreamed even if you are completely greedy um this is a big misunderstanding of uh of Open Source this this understanding this is why I used to be uh I used to be into the GPL and I switched everything to MIT and it's because of this um so okay search always works you just don't always have the compute uh so there's this thing called aixi um aixi is a mathematical formalism for AGI and it is correct um it uses the universal prior and then it does the bment equation to search I mean sure why did they explain this in such a complicated way I don't know the one in that book is better the one in the the RL book I thought was much better where I B come why is that the bman error is not learnable what does that mean uh have I looked into basian optimization search yeah I I don't think this stuff's the problem well let's Google it quickly but again this is this is all old stuff too Oh you mean the hyper parameter search why hyper parameters exist is kind of broken at least they're not like ex just this alone how much progress has been made on this single part of the problem never even minding the fact that Transformers might work better there do I do a lot of probability Theory you're talking about a comma no no ml is an empirical science um we try things something things work better so the thing that we're working on right now is K released a lot of this stuff uh but yeah I mean comma has comma has two main modes and our Moes are our data extraction Pipeline and to some extent our training pipeline I I don't think if I were in a large uh company I would do exactly what like mistol did I wouldn't release my training pipeline um I think there's a lot of stuff there but again if you have some trick for training there there's no reason not to make that public and maybe it is all public and it just like hasn't gotten better um so we're playing with this stuff now uh we're using so this is a generation model our new generation models look a lot better than this one uh so this is generated in a simulator and then you can drive around in a simulator and then we found a way basically if you pin the future we should be able to use the human ground truth let it deviate bring it back ground truth into the human ground truth and we think that that should work it's not perfect we haven't found a perfect a perfect expression of this stuff yet but it should be better than what we have it's interesting that it's called beta here too for I doubt this torch RL is very different from from mine is there a set is there a test is there a set somewhere of gym Suite of gym tests uh gym environments minimal RL test environments grid world oh I kind of remember grid world oh five walk zero let's let's just try this can I do environment name equals 5w z it's going to just work I don't think so I have to install classes fir walk does not exist we're going to have to install this uh isn't BS 256 high for card Paul yeah probably is can I make it less doesn't work but yet if I set it to 256 it learns how a cart pole at least a little better wow this thing really sucks can someone tweak the hyper parameters in this and make them good this is this doesn't work like we just had not that time it worked why did it work that time oh and then it lost it it had it for a bit and then it lost it it it feels genuinely worse now that I fixed that uh now that I fixed that bug in p and like the bug wasn't even it was just total and complete garbage that I had okay well that's what you're small see stop see that's this is exactly what doesn't work this is exactly what doesn't doesn't work just keep trying to spam things which is kind of frustrating because this is my usual style of programming which is just Spam programming should you put grid search overnight yeah if you want what if I pip install gym Classics does this work of course it doesn't work I I have to import it and register it somehow JM Classics register gem great no module named gym oh CU it's gymnasium oh this is so frustrating oh maybe I can register the gymnasium of course it doesn't work well we try see you learn that time kind of and then I forgot at the end classic grid World zero has 11 states and four actions what are all the environments in gymnasium I don't think there's Mountain car it does not learn the mountain car oh they have a Flappy Bird environment why is the entropy loss going to zero so quickly didn't they say that that was a bad thing also notice note that the reward is always -200 so Mountain car seems to learn nothing how do I get any reward at Mountain car let's increase the entropy scale so that doesn't go to zero so quickly why is the entropy loss going down that fast did I flip a sign in my entropy loss I feel like it was better before I probably should have saved what I had before no my own card just gets negative -200 on every episode and the entropy loss goes to zero that seems like a bug oh we're not doing dqn we're just we're just like this is supposed to be a Theory stream why does this stuff suck so much po is also a lot slower than some of these other algorithms oh Flappy Bird a lot of people do Flappy bir looks like so this might be one of the struggles as well like the data depends on your model uh which then is used to train the model which then changes the data which then with an off policy offline RL well so that's what decision Transformers are um yeah any methods that look like policy gradient methods are going to open AI still wrote the best stuff on this come back to this a few times why is there not why is there not better stuff I when I I I read this when I lived in San Francisco am I just missing stuff I I hope you guys will link it in the comments if there's any new stuff like all all of these methods are now 5 years old and they all suck um we'll occasionally get papers like learning to walk in a day or walk in the park this is interesting like he's famous uh this paper is like a year old now you say it's so obvious that RL is the future well sure but it does doesn't work like language models just work language models you could just scale and they work they're pretty easy to debug um maamba versus Transformers is the hype real things like Mamba might end up being good I kind of feel like Transformers are the lstm and mambas are the grew where they may be slightly better but in practice everyone's getting LMS okay a library for deep learning and reinforcement learning I don't know what this is who made this 25 Stars probably not real multimodal stuff so some basic stupid uh some basic stupid LinkedIn to your shit no no they're not rhf is not that great um I do they use PPL like what do they use they the other thing about rhf that kind of uh it's very it's a very weak fine tune um a lot of people are using this direct preference optimization formulates a reward function based on the human preferences and directly trains large language model to maximiz this reward like like it's not this is the new one people are using yeah but in some ways is this not even what's the difference between this and just training supervised learning on a high quality decept for Downstream task of of Interest second phase the sft model is prompted for preferences optimize the language model to minimize ldp a lot of complex stuff for we can read the we're going to we're going to make things are going to look good in tiny grad like this is all just boiler plate where's the actual one thing that matters um compute the DP loss R jef aims for reward maximization with a KL constraint what's a KL constraint DPO drives a loss on the current policy where I know what kale Divergence is yeah but like what's a kale constraint DPO derives a loss on the current policy where our data set says this is preferred to this oh it stops IT drifting from the reference model oh okay I see I see so this is just that makes sense okay this is this is very understandable uh literally say it's it's a this is the same I feel like this is the same trick they use in in stable diffusion with the uh what do they have the the the guidance it's like the same basic idea I think but you have this is your and this is your no and then you also constrain them both to the reference model what's this Sigma the learning r log sigmoid beta times logits decision Transformer outperforms PO rhf on my fine tunes how are you using decision Transformer on a on a language model all right so this is the latest deep mine stuff my name is deep I feel like still publishes a lot of stuff on of my team literally inserted in the prompt is there paper on this how do you do that scaling up robotic Transformers with self adaptive robust attention H I'm bearish on this paper already this doesn't look like one of those papers that gets lots of citations for a long time whereas like decision Transformers looks like the kind of paper that's going to get citations for a long time um and then Schmid hoer is going to bitch about in five years he's going to be like decision Transformer just stole upside down RL um I literally invented that and you know what he did invent it um is that a monzal deck it looks pretty good yeah this is not okay let's see blah blah blah blah blah a regular adaptation model computationally efficient performer is great nobody cares reward reward text do you have a link to this paper is this like pre- promp thing if I let nonsubscribers talk are you guys going to say anything useful like we're actually having a real conversation here today um this is this is a learning stream yeah I don't this is this is TX for fine-tuning llms uh po what's implicit language Q learning offline orl for natural language generation for for I don't know about this looks complex all all the best tricks another paper using diffusions to generate trajectories oh yeah I saw this one this was pretty cool this is the one where it shows like yeah yeah yeah the one with the T where like it goes around and diffusion actually keeps the multimodality um yeah if you usion policy keeps multimodality whereas these guys don't um I think we played with diffusion policies a bit at comma and we didn't really find uh Improvement our policy is pretty good so we use uh a multimodal gaussian like from like the old days and that's generally not the problem um our biggest problem at comma is cheating in the simulator our simulator is not good enough so we're hoping now that the simulators that we learn in Transformers will be better um the simulator is hand coded it has a lot of hacks uh so you need a simulator that you can initialize from any real state so then you can use the real human policy uh aligning language models with offline reinforcement learning from Human feedback okay supervise F tuning just the normal loss [Music] function fine-tune the sft model on their Bandit environment using p Toyota recently used this to cook food whose paper was this by the way oh this is the toyot one okay offline reinforcement aims to Le an effective policy a fixed data set collected by Behavior policy without any interactions blah blah okay decision Transformer yes yes yes on the hidden stream where the incident occurred uh we tried and failed to implement this decision Transformer um because the incident occurred twitch did not make the video available uh so if you missed yesterday's stream I'm terribly sorry but you know you'll never even know what the incident was so uh yeah that's right that's that's why you got to tune in every day uh every weekend even if I move off of twitch because twitch only gives you 50% there was some good stuff on yesterday no there really was some good stuff on yesterday's stream too like we really went deep um sorry I'm not paying attention what is this this really lost me when it started telling me about how it was running the nodes which I just don't care [Music] about would you stream on X bro you're a subscriber I still don't understand what this is oh I see you kind of just like put the reward in you just put the reward in as tokens I see this paper is going to end up getting a lot of citations because it's just it just has such a simple like there's nothing more beautiful than this idea um thanks Schmid Hoover people are moving away from RL to imitation learning oh yeah I've spent the last seven years doing imitation learning comma is imitation learning um can tell you about imitation learning and yeah it's it works to be fair this is this is why we use it open pilot is trained entirely using imitation learning but imitation learning can obviously never do better than what it's imitating I don't know this might also be a problem of initialization but I think we've played with this at comma too and this turns out not to help that much but so we can talk about why imitation learning is is is better uh imitation learning doesn't have have this problem imitation learning doesn't have this problem and imitation learning doesn't have this problem well it kind of has this problem still but not exactly like there there's still this this unfortunate feedback but the feedback doesn't happen at train time it only happens at like runtime um but at least you get rid of the non-stationary of the data uh you get rid of the credit assignment problem just literally it's just predict the action it's it's just I feel like so much of this stuff has to do with the initialization tested in deterministic mode with the fully connected [Music] Network oh efficient zero was this the oh this this is the one I was thinking about before the tari 100K Benchmark uh dreamer V3 I'm forgetting exactly what they did too I I read all these papers back in the day I kind of forget um wow efficient zero is still the champion on [Music] this I want the Tiny Box to be able to replicate this what's the Atari 100k where's the paper that introduces this 100K step limit that's not that many steps should we add a bounty for turny two hours of real time gameplay okay $1,000 bounty in tiny grad let's go I'll add it right now solve Implement efficient zero tiny grad solve Atari 100K all right $1,000 to whoever does it this is the best imitation learning paper you've seen what are they doing oh they wait they they actually can you do dreamer V3 um is it better than how does it compare to what does this do oh World model learning oh that's cool okay sure we'll just say Sol Atara games in reasonable time $1,000 oh yeah no I Me Dreamer V3 is just it looks a lot like what's the difference between dreamer V3 and perplexity nose we just Googled to find perplexity oh I remember that I remember that being the the big is a model based algorithm oh Mew dreamer what's this man I love perplexity oh now we're getting somewhere oh this looks good learning predictive World models without reconstruction oh that's fire cuz okay they they have a they have a reconstructor this is this is most similar to what we're doing at comma Now um except we don't use actor critic learning we use a different kind of learning but our world model learning is uh is similar what's JEA I I remember this from like way back in the day I don't know about that I I this learning predictive World models without reconstruction the dreamer V3 agent based Bas turns out it's three guys on Twitter with EAC in their name no I don't know okay however the Reconstruction laws is essential to dreamers performance it also necessitates modeling unnecessary information this is exactly the problem we have at comma consequently yeah Mew dreamer learning a predictive World model by the way I love perplexity I read lon's long thing about about uh how how these agents are going to work and I don't know show me the code uh I think I think it's the kind of stuff that you have to do like I've written like similar levels of of abstract uh like you can read this like this is my level of really abstract stuff for comma um okay so how does it work MW dream oh God a reconstruction for you return to World model lat in Space by predicting not only rewards oh God what was that paper did this paper exists way back in the day some somebody somebody wrote this way back in the day I saw the year I was at nips um muo cited it I remember I remember I went up to these people and I I saw their poster um whoever was at the poster knew nothing about it it has a name that's like hard to remember but I'm pretty sure it was it was Berkeley no some like cutesy name they open sourced it too oh I think it's called like value prediction networks get this this paper again six years old now six years old I felt like they had a lot of the right ideas like way back in the day um and it looks kind of similar to to M dreamer let's see if perplexity knows no that's not really true this is how you're learning your reputation your your representation space Oh I didn't realize there actually was code for jepa joint embedded predictive architecture yeah no th000 bucks if uh uh I'm an even to say or similar dreamer V3 efficient zero or similar in tiny in tiny gr uh I'll say just solve Atara games in reasonable time I want to solve Mario you know what uh I think we could afford a researcher at at at tiny Corp you if someone wants to come work at tiny Corp and wants to basically do nothing but you you in you know you have to use tiny grad and you have to use tiny boxes but if you want to come work on uh the thing that I really care about is the uh wall training time not not not anything else I I don't really care about how much compute you use I don't really care about um I care about the wall training time being low so you can quickly iterate and I care about the uh code being short other than that and use tiny grat use tiny grat write short code and make the wall training time l and uh just come be a researcher tiny Corp and work on that stuff um yeah we raise money uh I I'm not going to look at your citations uh I'm interested in fact if you're interested in the job solve that Bounty um solve that Bounty everyone who gets hired at tiny Corp comes in through internships um are you you bought a tiny box so that's cool uh if you send me an email by the way there's a few people well you know what work on the Bounty people who have contributed to Tiny grad I will prioritize and get them their tiny boxes first or people who are generally using it people who I expect to put good GitHub repos up um the tiny boxes are coming soon we we're uh oh you're algo man are you the first order oh yeah you'll get yours in no time anyway um we are almost there you know I have tiny boxes uh I have tiny n tiny eight here are my first two tiny boxes oh these two are in the common data center we have tiny S as well but here we can check them out uh they're also going to have how much are the tiny boxes uh 15 grand no no no no no no the tiny boxes all the Nvidia cards are in the compute cluster the tiny boxes are in the data center uh because they use AMD cards so yeah you can see 32 cores 6 7900 XTX um I'd love to see a great version of like dreamer and stuff written on these uh no it's not why did you think it was that why did you think it was was was one did I talk about this on stream um they're too expensive still uh this this CPU is cheaper you don't really it's not that much faster yeah it's super cheap 1K I'm paying well those went up in price to 500 now but I was paying uh 370 for them oh yeah tiny boxes are way cheaper than we're undering lber Labs 2x at least um that's T bucks is is is is actually a pretty great deal I don't make that much money on them there's about $10,000 in Parts in these uh but I think it's really important that we have a reproducible machine that's capable of training pretty quickly you're not going to be able to train gbt 3 and stuff but you should be able to train Atari we should be able to train good enough stuff that all the basics can work you should be able to train a language model that works an image model that works um then you can experiment experiment experiment on your tiny box and then you can use well the tiny cloud in order to train really big things um how am I thinking about multi Noe well first we're getting multi GPU to work you can see the multi GPU stuff that shipped in tiny grad guys I I just love this stuff I I'm so excited uh I was I was bummed oh you you guys missed this stream yesterday I got I got really frustrated yesterday uh I swear I didn't delete the stream I like to you know I like to unless there's something that's like related to like my security I I everything about me can be public and that's it's uh I I I I reject that characterization firmly um that I deleted the stream uh because you know yeah I got frustrated but that's kind of the point it's it's through frustration that we move forward um like we got to just no we need better tools we need we need better Frameworks we need stuff that's just easier to debug uh I'm gonna I'm Thanos snapping on Tiny grad tomorrow I'm deleting a whole lot of crap uh we're we're I use the word Thanos snap I never actually watch those movies so I don't really know what it means uh if it means like we kill a lot of people that's what the doomers want to do with AI That's not what I want to do with AI um oh a bounty up for a fast P scan uh yeah I think yeah if you have oh and if you want to pay the Bounty yeah absolutely just like put an issue on Tiny grad and say you'll pay it and I'll I'll promote it with the other with the other bounties and I'll call them like third party bounties um the Thanos snap just doubles the resources for everybody alive well good wait that's exactly what we're doing here we're doubling the resources for all the code that will remain in tiny grad uh yeah if if you just put an issue up I I'll promote it okay uh let's not get distracted by I let's understand what this is Mew Dreamer is it I I I understand Thanos was a bad guy but what do you mean isn't this isn't this the the the dream of leftists everywhere like isn't this what the great reset is Marvel life imitates art uh all right I don't really understand that I thought I didn't have to have a the decoder gradient is not back propagated to the rest of the model the hidden representations are learned solely I feel like this is just I feel like this is just value prediction Networks all this stuff's the future okay don't ever let me get caught up on stupid RL again uh we just we just have to not do these tiny problems we we we wasted a lot of time like re you death stuff these things just aren't problems on on on bigger things but then like the bugs are harder to see in bigger things but that's not really true so one of the things that's really cool in in in tiny grad or it's not even tiny grad's idea it's it's this um this lets you iterate really fast on a model like the whole model trains in under 10 seconds Me Dreamer Loops good your dreo looks good uh so they still have a reconstruction I don't exactly understand this maybe I don't understand how dreamer dreamer uses the de how does it have a decoder no no where my dreamer V3 go oh here V3 okay huh oh I see okay I get it that's cool I feel like we can do this at [Music] Ka unlike dream the Dakota gradient is not back propagated through the Wern model the hidden representations are learned solely through value reward episode continuation action prediction heads you want to come in say hi to the stream why don't you want to come in you're not dressed appropriately for the stream I have pant what oh that's okay it's I mean it will be nothing comp do you know about the incident from yesterday's stream what happened do you know yesterday's stream doesn't exist and didn't get archived because of the incident oh no don't if you know what the incident is don't tell them I heard what you did on I know I know announc it and I got a warning I got I oh you were there for the warning when I when I woke up when I checked you were streaming it popped up oh wow Alex was Alex knows about the incident um that's that's really messed up didn't they even record the archive no no no no we're we're we're moving off twitch do you to like um twitch doesn't support my alternative lifestyle I thought by alternative lifestyle you were like gay I didn't know it meant that you like to sit on the couch and smoke weed all time well yeah but that's like your problem I'm really sorry that to you yeah do I want a Pok ball or do I want homemade pasta it's your choice was it like the pasta from two days ago y you like that it was good the K Pepe yeah K Pepe yeah was good I can more like that all right that that sounds nice thank you no you can't you see this morning stream you don't see yesterday's stream yesterday's stream is the hidden stream uh extra p uh wait this is this is this is wild just like train the decoder but you were you DJ Cod rad was there for it right um yeah yeah he was there for the incident I'm interested in trying to do this at comma all right so the only thing here I don't understand is what this mkl is can someone explain this to me wait you're still getting a twitch warning about how this stream's not for kids does that reset this is messed up I mean this stream is totally for kids wow wow we really can't stream on Twitch anymore this stream is absolutely for kids no this is this is wild I understand it's kale Divergence but kale diverence from what wow intended for mature audiences after the the one terrible incident from yesterday wow I mean someone on Twitch someone twitch flagged my my stream that's what they they really are upset that I spoke out against Harvard um move to Discord stream no they they're they're they're not blocking your comments yeah no the after the incident yesterday wow between the previous observation and the current observation and you have to ask to get it unflagged wow wow no we got to we got to move boys we we got to move where are we going we're not going to kick I'm not I'm not a crypto scam person uh we could potentially move to Twitter mixer Rumble no we're definitely not moving to rightwing yeah I don't know I kind of don't like supporting any of these platforms it's kind of upsetting that I'm supporting a platform make a website yeah that's so much effort and it like won't work and it won't be kept kept up to date I don't know it just like it makes me it just sour me towards twitch entirely Twitter is an option no moderation of chat uh hotar TV I do own only george.com no I I don't support crypto gambling I didn't do anything that terrible on stream I didn't do anything illegal in the state of California guys uh no but this I mean this does upset me if if twitch doesn't reach out to me and unflag my account we're done and offers me a 7030 split the 7030 split is fire the 50/50 split's not doing it not not not cutting it for me anymore I mean yeah but it's not going to cause you cancer oh I guess I could influence the youth I could influence the youth to you know not go to Harvard kick does 955 really no don't tell Fred Rico what what happened what the incident was um it's $60 an hour to use AWS media live God the K term is between the predictor and the stochastic of the representation Network what then we're going to Facebook no I was actually I was in a really bad mood after yesterday's stream and I went on Instagram reals and I found some of the most hilarious Instagram reals and I was cheered out there was one of a cat being put to increasingly small spaces um I don't understand what you mean the predictor and the stochastic c section 4.1 we'll explain the learning okay if I expose my butt cheeks on stream will that get twitch's attention okay sorry I'm just clicking and I'm not reading sometime I do that no I understand what kale Divergence is but what is the K Divergence between they put me on the front page am I on the front page right now what the hell are drops no I'm not on the front page right now equation three shows what it's between okay h God I don't follow that okay the representation Network oh in the Dynamics predictor oh okay that makes sense I see why is sebas like lus Tech tips uh lus Tech tips they reach out to they reached out to comma they are so entitled um just like not the kind of people i' want to work with oh they reached out to comma they wanted first they wanted free review units and I'm like no we don't do that we have a 30-day return policy you're welcome to buy it and send it back and then they like they asked a bunch of questions that were just just like really easy to Google like they were just things you could have Googled and I'm like bro like Google it um and you don't know reply from them they're just like they want you how could you we're going to bless you with coverage and I'm like yeah you saw what Elon said to the media we can't say it because we might get another twitch flag um didn't they auction off someone's stuff they were allowed to review or something look I mean yes as far as I know that did happen I think they apologize for it I think Mr mistakes happen there was also like the screwdriver incident and the warranties like they do make mistakes but again I can really only speak to my personal experience with them and the guy who reached out was incredibly entitled uh so you know I'd rather not be covered on your stream like I'm not going to suck your dick to be covered on your your shit bro um they should unroll which multiple steps oh unroll the yeah I'm not really sure what the what the hidden State thing is like is that is that a Transformer I mean really what you want here is to for this to be a Transformer and then you can use I wonder if you can use just like half of vqv well just imagine imagine a uh a Transformer here and then just the front half of vqv but so the same as I'm telling you this was all kind of done in in value prediction networks seven-year-old paper now like they they just train on the the reward uh the value and I don't know what uh I don't even know what letter that is what letter is that gamma is it a gamma it's a gamma the discount I don't know why the discount factor is output from the function but whatever um it's basically the same thing what's continue we in a world model the world model is interested as a recurrent State space model oh sequential network using a Gro yeah I think these things will work way better um yeah these things I think will work way better if you don't use a Gro if you actually use a Transformer here or some like yeah like a States space model I mean isn't that what M was called like a States space model oh I see it's episode continuation okay that's that's fine um so yeah they're doing they're minimizing the K Divergence between the prediction of the state and the encoding of the state the thing that so I always think about how to train these things without without uh putting the gradient through the decoder and I worry that they'll collapse down to uh down to nothing imagine an encoder that just outputs all zeros and then the KL Divergence between uh two sets of zeros is uh nothing right so that's that's zero so it really is these things that have to learn all of the uh all of the stuff you prefer the I JEA approach more than the distribution matching this this reminds me of when everyone got hyped on like contrast of models and stuff I don't know I like vqv better than all this yeah like this is this is still going to suffer from uh look at the predictor well it just like predicts like a part of the no I I don't like that it's just like it predicts a part of the image like okay fine uh but that that's not like this is this is a hand engineered feature for images that happens to work I like vqv better than this so at comma we use we use a vqv to learn an encod or decoder pair and then we learn a Transformer on the hidden State um you could do it over the sequence length yeah but I could also just do ah H I wonder what Comm would learn if you just told it to predict all the actions like if you just if you just roll this out until to predict all the actions you're right you can do this multi-step too uh I'm I'm not sure it really matters one of these things is going to get so good that figures out how to use a Transformer instead of these other uh uh look up table free encoding not sure we have this fsq and I don't I don't know what these are uh curiosity driven exploration this is an old paper yeah I read this way back in the day which lab is this bery did a lot of cool stuff they encode only the features with predict res actions taken yeah that's that's that's right it's the same stuff here this is literally this is literally value prediction Networks for are these images that it's reconstructing look up fre quantization wait how do this look I still need I still need a stupid commitment loss still changing the code book I don't understand how this is different so I take back everything I said about things not still being published things are totally still being published just no one ever got better at these basic RL tasks uh uh signs it to a local thing in our closest codebook improving reconstruction we reduce the vqv codebook embedding Dimension to zero I think that's the wrong link this looks like a different one uh with a simple scale we project the VA Dimension down to a few Dimensions each Dimension is so wait but it's not this then okay this this is another trick you can use uh yeah I mean you want to of course you want to remove these auxiliary losses this is great that uses all code words without fsq you get an implicit code book of almost any desired size consider a vector Z with d channels if we map each entry Z to L values oh wait yeah oh we're just going to change we're just going to use as a comma that's that's yeah if this works or if this works this is so much better yeah yeah I love this no it's just so annoying I that that hyper parameter sucks uh people talk about this mask yet I the dream of the dream of tiny grad research is to generate uh is to solve like I like to solve Mario 64 I I want a tiny box that can beat Mario 64 how has no one uh was one of my favorite games of all time um no so this is this is representative game playay how come AI can't all this I beat him when I was [Music] seven [Music] all right I think it's dinner time I think I'm out I think uh I'm definitely going to bring this up at comma tomorrow uh I'd love to switch off maybe they already did it I don't think so though I think we still have the cook thing um we're running into a problem now where we need to make our simulator higher Fidelity in order to be able to use it and we run into exactly this problem where we make the code book bigger and like it doesn't get that much better um no it's not a copyright problem what do you mean I can play the game or whatever solve solve some 3D have any of them Gotten Good at like like counter strike and stuff I don't know I mean Nintendo can sue me if they're upset that tiny boxes can play Mario 64 uh we'll add them to the list now I'd love to solve Mario 64 um if you're a uh if you're a researcher and you show that you've made a good effort if you if you're a researcher and you solve that Bounty uh whoever solves that Bounty uh do we run the co train the codebook simulator auto regressive uh no no we separate VQ vae and Transformer training uh it's just computationally a lot to do them together uh so I comma right now we're just we're just training a big vqv we get an embedding space for the driving footage and then we train a Transformer um we should post I think we posted some some videos on Twitter uh the new simulator looks very good it still doesn't look as good as waves I mean wave just poured a lot of computer to it uh no this is some this is some crap uh no no that's not real VQ up with an unfrozen version just do a few gradient updates yeah I'm interested in that we'll get there um yeah like they're thinking of all the same stuff it's simulation that can deviate wave is one of the few uh legitimate uh companies in the self-driving space um I I wish they' open source more stuff uh it's also clear that this simulator isn't a true neural simulator I mean a true neural simulator would never blink like that um they have some they have some hand coded rules on top of it uh Tesla simulation looks to be just straight up Unreal Engine like they they didn't do any any sort of neural stuff uh training oral and representation is the future I agree um oh no ghost ghost Locomotion is is off the charts idiotic they they they their strategy is copy comma from multiple years ago get 10x more funding and somehow manage to produce nothing uh so why am I not surprised that ghost funded this is not ghost this is wave and I've said good things about wave I've hey um you are not going to find someone who more accurately called self-driving and the best thing is it's all documented uh it's it's all documented I've been saying the same stuff for many many many years um I believed the bitter lesson before there was a bitter lesson uh and I think that's a good place to leave off on this stream uh so everybody if you have not read this uh please go read the bitter lesson uh and I'll read the first sentence out loud the bitter lesson that can be read from 70 years of AI research is that General methods that leverage computation are ultimately the most effective and by a large margin so don't hand code your feature spaces learn them and learn them with uh if you can figure out how to just just I can use round instead of having an explicit code book that sounds sick uh we're going to check that out at comma ,000 Bounty up there and if you are interested in solving Mario 64 um on a Tiny Box using tiny grad uh we can fund a research position uh I'll pay more than a PhD styp end uh we also uh know it's a nice environment to work in uh so I think we can totally do that and if anyone's interested all bounties thanks for watching tonight's stream this has reord restored my excitement and faith in reinforcement learning the truth is you cannot reinforcement learn a human could never learn anything if all I was given was an eight dimensional Vector of lunar lander you have to give me a picture you have to give me a high dimensional space these things work but only when they're big and they don't have bugs thank you everybody good night I'm going to eat some Pepe Pepe yeah uh if I leave twitch uh if I leave if I leave twitch where will you find me you follow me on Twitter I want to post on Twitter this year but we already got into a beef with twitch about the incident from yesterday stream the Lost stream you'll never know good night everybody