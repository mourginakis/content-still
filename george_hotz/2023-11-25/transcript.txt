good morning everybody good morning good morning good morning it's nice to be back in my house I don't like streaming at work uh high ball energy good morning everybody good morning good morning good morning good morning all right let's uh shrink me down here me up there oops don't do that okay are we ready to get started um yeah felt Vibes you a little bit off in yesterday stream uh I don't think I should stream at the office oh so I didn't like the way that like like now this is here let me slide that down a little I'm about in the middle that's pretty nice I got my Yeti microphone move it over a little bit Yeti puts me perfectly in the center uh yeah Vibes are Vibes are off wouldn't have my lighting set up I it's an okay stream that's good to focus on tiny grad but some guy said that he liked my older work better and if there's one thing I never want to be it's Drake so uh we're going to try to we're going to try to bring it back to the old stuff and we're going to do uh we're going to investigate the qar algorithm so first off what is the qar algorithm I don't know let's find out we'll start using our friend here Google what is qar and when will we hear more someone who's done a fair amount of ml research I can tell you it's very very easy to think you've discovered a breakthrough that's true all right let's read this article in The Verge uh Reuters okay well the government no no no no no no no uh qar could be a breakthrough in the startup search for what's known as artificial general intelligence Reuters could not okay it somehow does math do something I still publish papers not really wow remember when they used to publish papers and now they publish system cards and and safety and Alignment okay wait wait there's some hope here improving mathematical reasoning ooh o this sounds like qar guys I think we found it no wait no no no but really wait is this not is this not qar did we not just find it we're trying not to hate we're trying not to hate guys there's so much hate in the world we got to bring love and positivity and shit man whatever like John lennin said you know something um that's why we're going to go subscribers only okay given vast Computing resources the new model was able to solve certain mathematical problems improving mathematical reasoning with process supervision we've trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning instead of simply rewarding the final answer and it just um okay so the reason that you usually just reward the correct final answer is because that's all that's in your uh data set the other problem with doing this is it may constrain the reasoning to follow one certain path when that may not be the path you have to follow stateof the art models still produce logical mistakes often called hallucinations we good here we're good cutting out we having more technical problems I bought a new computer does new Apple stuff not work oh only you okay Rick and Morty you have a crappy internet connection sounds like your problem uh no no no no positivity positivity positivity you have a an internet connection that is less good than other internet [Music] connections wait do open I just publish like fake papers now like this positivity positivity positivity positivity um don't give a shit about alignment wait this is a paper oh okay no here there's a paper let's verify step by step okay um okay well is this a public data set all right all right all right all right cool math word problem solving on math who made this data set okay okay this is not where I was expecting this to go by the way oh they have metamath oh everything comes full circle no I mean again let's okay let's find the primary source here qar it definitely talked about math problems only performing math on the level of grade school students why is this monitor broken this monitor is like broken I got to a new monitor you guys aren't seeing that glitching right are you maybe you are I don't think you are this is so weird when I click here there's like a blue glitching on my computer it's weirdest thing like how does that even happen happen I could take the camera down and show you you want to see there's like blue glitching on my monitor look at this click here goes away here glitching here goes away glitching how's this happening has anyone ever seen that before you see it it's a ghosted image maybe it's the cable I don't know right who knows it only happens when things are in certain wait this is just really who knows all right uh oh wait I wanted to confirm that this actually was related to math delti cable breaks I'm using like the USBC cable it could be the cable but like it's weird how it's glitching I don't know um the new model was able to solve certain mathematical problems so it would kind of make sense that this is the data set right do they mention the math data here okay our process supervised model solves 78% of the problem from a representative subset of the math test set we also release PRM 800k okay opening I released something look at that I don't no positiv positively positively why do I have two vs codes it's all also look into what their other GitHub activi has been procedurally generated game likee gemm environments what did they update in gpt2 is there a branch they [Music] updated quantifying transfer and reinforcement learning okay it seems like Carl cob has been if anyone invented qar it's Carl cob Carl cob all right you been working on this math stuff for a while let's see what data set was used here oh is this the data set that introduced that oh this is a different one GMS 8K PRM 800k step level correctness but the glitching is really bad okay wa wait let's talk about the glitching for a minute somehow you can see what's here right and it's this it's the same thing that's in this terminal window but yet if I minimize this terminal window it's still here oh oh we got a we got a good resource we got a video Let's see getting r roll here you're a subscriber you wouldn't do that as you might expect I have been researching nonstop about powerful AI wow glad glad we got other researchers on board guys qar on no this is this is really terrible hang on I'm gonna shut that off for a minute and you guys can look at me while I try to fix it to the monitor oh come on is the stream working again oh I lost internet too oh I lost everything when I turned the monitor off wait is this still working it is working okay I don't know the glitching is just really bad it's usually not like I've seen this before but it's gotten way worse no it didn't help wait all right so they're looking at the same paper I am computing power while you're Tak all right let's just read the paper I need to watch some YouTuber read the paper oh the optimal Q fun possibility and he said one link to the name qar could be in a generic sense generator the model coming up with Solutions with reinforcement learning we do not dis paper using test time Compu okay um you know what I think maybe we'll do one of the Bounties in tiny grad I think we're going to first need a chat model I think regardless of what we're doing we're going to need a chat model I was playing with trying to make edding fast last night connecting process supervision to 30X model size we got a whole Army here you guys can watch this video so I don't have to throw that up there I did not realize how broken this monitor was all right so first we're going to need a good model what what's what's the best model we got what's the best uh the best 7B model we got is it Intel neural chat is this one nerfed to hell do you think open Hermes 2.5 is that what people like I'm locking the Bounty for myself too so I'm going to go do that some guy submitted a poll request but it was literally just like the words like why you even submit a poll request uh okay is is this what we like wait why this one doesn't seem as good now Hermes seems better that's 70b but why would I not use Collective cognition oh wait you can't see my desktop what wow we're having technical difficulties oh I think we also might have a bit rate problem no that's a pretty high bit right okay we'll put that in front of that okay all right all right all right we fixed that okay can you see now uh wait why do I want want this model it doesn't seem as good fine tuned Orca DPO do not trust benches you must try it let see all right all right do we believe in open Hermes we like this one okay um wait so which one is this a clone of does it have rope and stuff what's the vocab size so okay it's it's a mistal fine tune we don't actually have mistal support oh technium is okay people like Tech I won't lock the bounty to myself but someone's got to cuz I'm not going to do the testing but if someone wants to do the testing they can have the Bounty it's $200 um people train with data sets from benches yeah damn cheaters all right we downloaded in 10 minutes um let's take a look at mistel because first we're going to need a powerful if we want qar it seems like we're going to need a language model and we're going to need a powerful language model like mistl 7B oh no I don't know if I have sliding window attention did I explain qar well it looks like it's this or something but we're going to need a language model and it involves math so let's first see how good it can do language math and let's upgrade it with the qar algorithm uh use it with our reference implementation okay mistal Transformer I don't know if we have that you can look up in vlm implementation you think gbg 3.5 is way better than any open source model my theory about a lot of this is that their data source is just a whole lot better than like slim pajamas and stuff interesting oh I didn't know that they did this wait this is okay and only with three so you can actually look there's a thing called mask in uh in attention here and I think that they just changed the mask it's cool to look at these latest tricks um we Implement a rolling buffer cache uh oh I mean this is yeah only once we hit the context length what is the context length of mistal it's 8K sick file so it has two files is it not the seven BS only have one file so if I look at like llama 2 yeah they just have one how does this one have multiple I don't know what this other format is I me yeah that's great see this just has P torch model wait what why do they use fp32 we trust technium and open Hermes technium is like he's on Twitter we trust people on Twitter sorry on X okay the first one's still downloading there's two but they don't match in size why would anyone do this this cod's all terrible I let this get out of hand with the open source contributions Hermes is your favorite of the myal all right sounds like you've been playing with these things a lot um okay we're going to need to implement this sliding window attention I think we're not going to figure out what's exactly in the stuff till uh yeah these personalities are also like this is old shit you know what why don't we just rewrite it like I don't want to deal with any of this garbage this code all looks terrible let's right mistal dopy with the latest stuff Oh blank code yeah exactly if I want to use I oh I don't need sliding window attention wait I think I do or you're saying they trained it they trained it without sliding window attention for smaller stuff what do you mean by the window size where do you see all this stuff as far as I know mistel didn't really release a paper with each layer tends to the previous oh okay so it's not actually three here W equals 3 but in practice W equals 4K ah okay a sliding window of 4K I understand okay that actually makes a lot more sense than three and good we can we can use all the latest in uh in stuff okay all right um let's get these models loaded here so we're going to start really with with mpy here uh we we'll do stuff from scratch so in tiny gr now you only need to import tensor like that it's a lot nicer um import NN and then we can do nn. state do uh torch load because it's some kind of P torch model uh weight open Hermes part one part two I should really have this Mark tensors as read only just so I don't corrupt them accidentally um that let's run what we have so far so you see how fast that is by the way uh I worked really hard on this so our torch load function doesn't actually load the tensors into RAM it just loads them all with pointers um they're disc tensors so I can show you like okay oh the dtype is half which is kind of nice good so they're not uh it's not float it's not B float 32 so how does this work what's in part one and what's in part two okay good just goes up to 31 here okay um so it should have mostly the same architecture as llama I think that it's been impoved enough that there's not too much we can uh we can improve still well yeah I don't know man sure short your opening High stock uh do you just do you want me to write it from scratch like I I can just do it maybe we should CU it's kind of cool like does this even have a k q and yeah okay uh we're permuting them and sticking them in when we load the weights key map subk okay so where're where're oh this converts it from the hugging face format I say do we want to do that or we want to just implement the hugging face format what do you think chat should we use a converter or should we just rewrite it looks easy enough to write may you guys will appreciate it if we do some writing yeah yeah yeah I know about ggu so we can also print v. shape here all right that looks pretty cool so first we're going to need our self attention uh see if we can find those numbers they in the myal announcement or does everyone just know what these are for sevenbs now so the whole basically a layer seems to look like that let's keep the name's consistent from the original llama so this is called a Transformer block don't worry about what goes there later start with a Transformer block um again we'll worry about what it goes there later we'll just leave it like that for now but okay so we're definitely going to need something called self attention equals attention and we're going to need something called MLP what do I call it now feed forward actually why don't we look at the reference repo and copy their names it's always better to take names okay they do call it attention let's just look at there here Transformer block they call it feed forward too oh it is almost the same no look they call their stuff St this they just must have some weird loading script to convert it from the hugging face format too know I feel about that hugging face probably calls it something else then looks like the hugging face Transformers REO what did everyone use h vlm I've heard thrown around very complicated looking wow so easy you could just pip install this okay maybe we should import Transformer from llama just to make things go faster and we also can import convert from hugging face you can read it we wrote llama on another stream we could do it but you know we got to get to to actually writing qar some examples. llama import convert from hugging face import [Music] Transformer okay convert from hug face takes in a dictionary of Weights which is a G from string to tensor We'll add some types we love types no no no we're implementing qar um okay so we have a Transformer uh what's the dimension of this Transformer this is an INT all ins except for Norm eps what's one file ref okay there's something called Model ARS is a data class oh and then they pass it through with Json where's the Json is there Json where does this Json come from is it in assets Json sounds like an asset no those are images what's in deploy nope wait so where the hell is is Json that they're loading oh at torch inference mode that's cool we set tensor no grad equal to true what's fire I'm not up to date what's all these latest things you can call fire on any python object oh interesting I like that um all right so where do I put the args where where do all those things come from oh I know where the Json is it's probably in here here config.js okay oh no it's B float 16 no okay let's manually convert these wait I thought I put oh I put them on Transformer block Lo multiple of n heads and layers Norm PS is a float okay so the dim is going to be 496 I don't know what multiple of is what is multiple of even used for I'll feed forward feed forward takes in a multiple o to to increase the hidden dim how does llama get them multiple of 256 it's probably a good number let's try it see if things don't load okay uh number of heads is 32 number of layers is 32 Norms is oneus 5 and the vocab size is uh 322 say model equals Transformer what I'm just going to put that in I should really just put that in beginning of the script to do python path equals all the time okay good okay um well it takes do that take time kindr as a helper called timing it's kind of cool you know I like things to be fast okay cool all pretty fast can add all prompt here uh load weights uh create model all right now we're going to have to stuff them in let's try convert from hugging face uh weights which of these is n heads n heads and nkv heads does this work okay that was absurdly fast uh we should check no okay this cuz this does not actually apply them uh we have to load State deck load State dick is in nn. state. load State dick okay assign shape mismatch 4096 1024 uh which one did I do wrong um it's a little Annoying that it doesn't print the name oh here attention WK weight okay so let's see I don't know maybe the dim is one or two four no but that definitely changed that so if we look at the Llama code we can see where that's being created uh n heads times head dim did I get the number of heads wrong oh that's probably right okay so n heads is maybe not 32 number of attention heads is 32 but the number of KV heads is only 8 let's try eight no that didn't fix that what is this multiple of thing I wrote this I'm sure I just copied this from other people it's always good to understand so this should be four times dim I don't understand why that doesn't match I would actually expect it to be that Mitchell uses 32 heads for the query and for and yeah okay but which one of these is supposed to be the KV heads right or is my convert from hugging face wrong or does our model not support that now it should unless does llama not do this does llama do something different nkv heads wait what about v y llama is different oh okay wait no here attention but is it does the feed forward have to change okay um it's failing on the attention assignment ah okay so I can pass in nkv heads here to attention again I must have just copied all this so nkv heads is here how far do I pipe it down oh nkv heads is here where does it come from oh there's a there's a named argument okay so we just need to add uh nkv heads equals 8 here okay that's progress uh this probably has to do with the multiply not being right I've seen those numbers before uh so where's their multiple it's not 256 intermediate size is here custom FFN dim multiplier I don't think this was written correctly let's look at this Cove and see if that's how they wrote it oh they just have something weird called Model arcs yeah they have just args hidden them this is stupid Tod do what is this what is all this crap why is this four times dim why don't I just yeah I don't like multiple of FFM dim multiplier this is set to 1.3 like this should just be hidden this should just be hidden dim I pass in really I pass in Hidden and multiple of what okay they're refactoring this U we have to keep the old stupid Behavior going to copy this weird crap up to here do these even have they don't have hidden dim so we'll have to add hidden dim to the programs umm dim multiplier equals model Arc sub fmd multiplier we'll make sure we didn't break it later see this is what happens this is the problem with open source people add crap to my repo and don't think okay now we're passing in an argument called hidden dim this is much more sensible get rid of multiple of hidden dim can go here a multiple of and then goes here that's fine we're passing that in there uh where is that actually being passed in oh that just go straight to linear okay good we've already deleted that crap wow that's a much more sensible feed forward I don't know why we put that logic in there okay all right good now we can just pass the hidden dim in right there and what was the hidden dim from estol uh that okay so now that's n heads wait what what is that no no 256 is what gets deleted I should name some of these parameters that's nkv heads that's vocab size that's uh not rope Theta what's it called Norm EPS uh and layers and heads okay cool does it load yeah it loads very slowly oh this is unbearable oh this is terrible just put that there okay the reason it's slow is cuz we haven't finished the uh takes 20 seconds seconds oh come on boys I don't have all day oh it's cuz we haven't finished no I'm using the it's not the GPU it's not the GPU it's that I have I'm converting to the CPU to load bf16 oh I can't wait 20 seconds every time all well what can I do about that oh I know what I'll do it should be fast I think yeah because we convert them all to float 16 and that's what's taking forever right now cuz we're not actually doing we could actually also do the math in B float 16 Mac doesn't use RAM for temp does it cached M okay 13 G's now let's see how fast it loads uh I have a way to like put things in I forget what it is though let's just look it's called load State deck oh yeah okay it's the same load State dict I'm using up there strict should be okay okay 2.3 seconds great um all right now it's inference time let's create a branch so I don't accidentally push to master all right we're going to need a tokenizer let's get a tokenizer oh I also don't even need this anymore so now we've made a copy yeah I guess I do need model actually not really I could just but whatever um let's get the tokenizer over here H tokenizer model [Music] wow 14 wow 6.47 gigabytes per second what does Jimmy apples do what is this I don't get it okay uh where are we we're going to do some inference on some models what's BOS ID what is self self. tokenizer equals token Z yeah we're going to call this talk prompt equals do you like chicken uh talks is probably good wow no one's updated that to the latest um just model start pause equals zero uh temperature equals I 0.2 is that a good temperature what's the default temperature what's the default temperature 0.7 is that a highight temperature or low temperature how do I make the loading so fast the loading should always be fast um I just cashed it you can read the code right there you didn't pay attention if you're asking me that question uh so [Music] also fuck are you happy uh we can do multinomial here which will choose a okay fine we shouldn't call it talk call it SP okay you down with SP does this give us a number no it's going to complain about a tensor all right fine uh multinomial realize. item I got to item it and actually I don't even need a do realize I just need a do item 315 boys 315 okay um this should automatically this should be jitted and stuff where does the jit exist the jits inside Transformer or no yes the jits inside Transformer okay good uh talks out of pen talk EOS ID yeah yeah yeah this is okay we'll just copy this this is mostly fine I'm going to have to figure out how to actually like encode these things for chat and stuff uh until wait where's my thing that prints the output as I go is it not here yeah this is not okay I don't even understand why that's there this is terrible this stuff's terrible no one's reacted this in a long time we don't need any fucking numpy uh start PA equals lento blah whatever uh that's stupid and that's stupid length outputed Okay we we got to set out outp put it somewhere output it equals user prompt user prompt I can [Music] go print user prompt if yes then you must have tried Big Chicken in your kitchen big chicken's a versatile dish that can be prepared in many ways all right are we happy overall oh I I'm outp putting that wrong um we should also do this better do you like chicken chicken is one of the most popular Meats in the world and it's not hard to understand why it is verstile easy to prepare wow okay good seems pretty good okay now we just have to figure out how to use these things as chat Bots get rid of that we don't need that there's like some like special tokens for chat Bots I believe I'm not buying into this that there was a breakthrough this if if this approach has anything to do with like supervising the middle steps it seems really stupid actually like it's a bad idea um Mario thank you for resubscribing thank you for being a 20 month subscriber loyal to my channel even when I don't stream um why do I have this this is dumb okay good that looks like a decent chunk of code um we'll create a little function called output nonlocal uh outp put it c yeah we should just copy this to here that why is that bitching what it's right there I don't get it is that how that works cannot access local variable outputed then no binding for why doesn't that work oh cuz I'm in m h [Music] fine do you like chicken how about a recipe for a homemade chicken dish that's super easy to make oh oh we have to get these things working like chat Bots how do we do this okay wait wait can we just read this comment is the qar algorithm going to be implemented with the comma AI maybe a voice interface to some kind of assistant I just I don't even know how to help you people okay I don't even know how to help you all right all right sorry I can't help you right you you want to see a voice you want to see a voice chat this is this is one of the demos someone's been working on I need to wait for that skull m has been working on combining whisper llama and vits into one super [Music] model I don't know the magical GitHub incantation to do this there is one but I don't know it for okay this is using a small llama so the chat output is not that great what oh did I fork this from mistol oh my bad yeah I for for okay fine what I don't understand why did that work um here and KV heads is this just [Music] broken or am I doing something wrong what did something change on Master no that's fine I don't understand this is like related to what I was messing with but like I think it's just broken all right let's see if it's a on line fix uh it has no argument KV heads okay uh no this is wrong wait this should have an nkv heads I don't understand it has an nkv heads right there is it not passing in the right model KY eror 7B okay there we go hello are you listening are you listening can you not detect end of stream anymore oh this used to work this used to work boys this used to work it was so good yesterday all right I hope everyone's interested in a long stream today listening listening all right let's read the Cod the qar algorithm is not fucking real it's [Music] clickbait clickbait uh how does this ever exit has it thr an exception I guess what how does this work how is this supposed to exit okay let's check out the older version let's try this one hello are you listening Stacy I need you to say something to me Stacy sucks stay are you a rapper of course I'm a rapper I'm a rap star tell me more what's your favorite rap song My Favorite rap song is called my favorite song mcari b what do you like about it I like the fact that it's a rap song about being a childood friend did you have friends in your childhood yes I have many friends in my childhood tell me about them my best friend is Kylie Jenner she's a reality TV star wow you actually knew Kylie Jenner I used to go to Kylie Jenner school I was in her class for a while what was she like did she pick her nose she didn't pick her nose but she was a really pretty kid that makes sense do you sometime wish you were that pretty oh sometimes I wish I could smile more and not pick my nose all right what do you guys [Music] [Music] think AGI right uh all right let's get back to work how about a cookie with chicken in it wait that's mad weird bro why you coming up with this shit we so far from it h okay so how do I like put things into chatbots chatbot style how does this stuff work like what what are the right tokens to use forgot Google's useless here special tokens map wait how do I like switch speakers you guys know what I'm saying oh here's the template okay okay here templates for chat models oh Auto tokenizer what I'm start how do I get I'm start is that like a method on here every meth every model has its own type mistal instruct was trained with these tokens but blender bot was not is this a real token inst is it actually just the word inst or is that a real token like is it just that oh based pie ID is out of range I use the right tokenizer piece ID is out of range oh well it was smart the first time oh no never mind it might still be smart PE ID is out of range what oh why is the vocab size that why is that not included in this tokenizer model what is 2 + 2 two what is 3 + 3 6 what is 4 + 4 8 now it's done okay it didn't exactly get 2 + 2 right I think I'm doing this wrong then I have an idea these aren't the actual tokens and it has to do with these secret extra tokens llama has some secret extra tokens too Marin thank you for gifting Subs we always appreciate that um yeah back to kindergarten shit guys our Q star can't even solve 2 plus two what are we going to do we're going to get some coffee let's get some coffee and then let's learn about secret tokens okay they're secret tokens hidden [Music] tokens nobody uses reserved tokens for instruct tuning you're precious for thinking that what are the tokens used for [Music] then uh wait actually guys guys we I'm being serious right now we need to stop when it outputed two there it showed that it wasn't aligned with us we don't know what the model's thinking guys the model could be could be taking over the world right now we we don't know this is this is we need to hire Helen toner we need to hire an AI ethics review board to review what just happened there we need to slow down we need to ask the seals if they're okay with rocket launches so thises this why does this thing suck okay what's s and slash S and maybe I need to go like this this is the mistal one yeah oh okay this is improved it's it's got ads in it oh open Hermes uses a different template all right all right let's see wait so is it actually the word IM start like is that just a word or or is it like a [Music] all right you got me something default chat template no that's llama I don't think that's right I think this is right okay I am start system we don't need a system message right now what is 2+ 2 ah Marin thank you for gifting more subs do you have a question if you gift Subs you get to ask a question all right I'm and I'm star is this is this is this really right like oh okay that seems like the best so far oh okay good I love have her Boose this model is wait this is actually laughably easy oh you're right I need a line break there never mind I'm locking that Bounty for myself this is too easy someone else should have done this they could have made $200 but instead okay um let's add a system message wait I should really like let me just write a little something to generate these prompts uh tle list tupple uh user what is 2 + 2 um encode prompt P4 KV andp uh okay R equals that Rend um okay so we're going to pend I'm start k/n the I'm n sln uh and then we want to add going to add Imart assistant to this go to the word user prompt like that code prompt okay why did you why did you not exit when you were supposed to exit did I forget more returns or something why isn't it outputting I'm and guys guys this is qar I think we found it no I'm not locking the Bounty someone should do this but someone should do a good job I want a good job on that Bounty oh okay here I'm end how come sometime it finishes the stream and sometime it doesn't maybe our temperature is too high let's try a less temperature yo okay zero guys why is it why is it going off into this language AI garbage or should I just stop after I'm end uh am I using torch or tiny grad this is all tiny grad I don't really understand this all right maybe we should add a system prompt uh you are Gary Gary is a useful no no we refer to used Gary what should we name him Fred Fred is a useful assistant did I spell that word right I did not Fred outputs the answer and stops talking all right oh all right all right fine we'll call him Q fine fine fine you are q q is a useful assistant Q outputs the answer and stops talking Quenton wait you know what you donated Subs you get to name him congratulation his name is [Laughter] Quenton I knew a Quenton once he was hanging out in San Francisco and some guys were on the street smoking he's like yo let me get a hit of that he thought it was weed and it was crack that's a real Quenton story I wouldn't make up quent story like that oh why would the system ask what the capital of France is we could stop at I'm end I think maybe we want to do that how do I do this in llama okay that's pretty good there a new line before all right let's jack up the temperature and Steve Quenton still reliable the answer is four but it didn't output imn that time it outputed EOS seems to reliably do that that time it did I end yeah I like is that a real token or does it actually just put it in like that like is this right or is that like a secret like like it can't be that it can't actually be [Music] this we can check if it's encoded on a single token yeah yeah yeah no it's a bajillion tokens add a trailing sln after assistant it shouldn't matter I mean I can but it doesn't matter no no no no no it can't be this it can't no no no but it can't literally be this huge multi- token wasteful encoding can we get technium in here all right how do we print all the tokens it probably is the secret tokens then I mean there's three extra tokens or I guess two extra tokens right 3200 and yeah if I was doing this that's how I'd do it let's just try it that has to be what the two secret tokens are right start and end this is what did I download open Hermes V shit G 4 qmw no I downloaded open Hermes this one it just came out fresh fresh yeah okay [Music] um oh you can download the tokenizer dojon um wait but I downloaded the tokenizer oh here special tokens map interesting okay imend is the EOS token I think I can feed these in somehow but I don't know about starch oh here we go look yeah here you have this tokenizer config.js okay it is exactly what we thought it was oh okay yeah yeah this is what we want okay never mind it's not as stupid as we thought it was uh okay how do I load a tokenizer config and sentence piece processor why isn't our tokenizer encoding them automatically because it's not in the model oh I hate goog look at this shit yeah okay one and two are not what we want um I we could just do this by hand it's not that big of a deal yeah but how do I add them what's pad ID nothing real okay no they were added the special tokens guys this is all well done technium is base he wouldn't he wouldn't he wouldn't do this do us like how did you decide that this was a good time to prompt me about Docker garbage thank you for gifting more subs here added tokens. Json okay okay we just need to figure out how to add those in um let's see if anything here is useful a knit model file model Proto add boss enable sampling this looks h all right whatever no but then it won't decode CU if I try like sp. decode and I pass this in it'll bitch and be like that's more tokens than you have pieace IDE is out of range I just want to add a token model file model Proto out type add boss reverse midun piece enable sampling do we need to write our own tokenizer adding token to sentence why do you think it's time for random questions why why do you think that that's this is an appropriate time extra options we can R oh here here use custom symbol okay control symbols how do I Define a control symbol no no no we need custom tokens Samuel were you saying dumb shit before too or am I getting you confused with someone else here sentence piece supports user defined symbols three thumbs down you can rewrite the model file okay what is this model file what is tokenizer model what kind of file is this data why is it not in there I downloaded it from here why is the why is the model not in there should be in there right right why do they not update that let's go for yeah I'm confused why they're not in the model too how to extend tokens dictionary yeah yeah uh you can rewrite the model the Proto is a DSL the model file is stored as a serialized Proto buff wait but did I not download the Proto buff one isn't there also a protuff one no okay maybe there's this okay all right uh load from serialized Proto okay okay okay okay we're gonna get this um so I have to edit the Proto file I understand oh here the Python tutorial for damn cricker to implement their tokens bro what are you even talking about BR Proto C Brew install Proto C oh we can Li Bitcoin this can't be the way wait so what is the hugging face tokenizer it's a good point why don't we read that code I'm sure it's open source um should we just write a tokenizer we could also just import hugging faces tokenizer I think we have to write the tokenizer I can't I can't trust sentence piece shit but wait how do they load the how do they load it added tokens decoder what I mean something's going to have to unpack the Proto file how large is this all right all right who who wants to bet over under a thousand lines oh okay okay not too terrible what does this depend on Google prot all right this guy is close to getting banned bro needs more fine-tuning to be helpful look man I don't care who you are you know what I mean but like you're either helpful to the stream or you're not helpful to the stream you see this is called alignment and uh you know you got to be aligned otherwise well what happens to what happens to AIS that aren't aligned I don't know he's researching bro he's researching all right um let's load the protuff from examples. sentence piece model pb2 wait what the hell do not edit okay okay I won't edit it relax I'm not trying to edit it let's read the new Proto usff tutorial for idiots a where did I get where did I get that from like here the Proto is a DSL don't like here we go good good good good ridot perfect um import all right sentence piece pb2 Dot model Proto maybe how do I load it from desk here parse from string I why do I feel like I'm having like a weird sense of deja vu that I did this in a previous stream OKAY spb2 model Proto do mpm mp. parse from string I don't know why that's not autoc completing for me uh all right cool okay so what what how do I actually like get like a python that I can type in what is it Dash e what am I thinking of I get it not to exit what's what's the flag for python to do this all right this guy is banned baned please write about me please ban all right see he doesn't realized how this works you see I have a band button he has an X but the band button and the X are not the same Dash I yay maybe that's right I think it's Das that sounds right interactive all right sweet sweet thank you Smurf that's why you're a be VIP and that's why Samuel is banned I know I know he just he just didn't realize how that this works like like you have an X I have a hammer you can use your X and I can use my hammer we have like different tools okay uh uh mp. pieces. append come on a man can dream right what type is this okay mp. pieces. append ah spb2 do sentence piece has no attribute sentence piece wait but but I don't understand for wow it's been a long time it's been a long time I just blocked him I didn't ban him at first it's been a long time since I've had to had to really take the hammer out but you know it was time okay all right ban me guys he's calling his boys up at uh at the media they're going to they're going to write hit pieces man oh no Mike wless run what's that from I think it's in model Proto or something aha we found it okay piece equals I'm start and we have to give it a score what score should we give it zero that's a good score mp. pieces. aent based all right okay let's see if this is going to work uh we have to do in the right order I think end comes before start right now we have to MP do oh now I got to figure out how to right to the file all right we have I'm end and I'm start this is great this is great the progress we've been making is great why is my score oh he's a PHP student that that makes more sense [Music] um wait so how do I write it I don't know let's read that Noob where's that Noob Proto Buffs tutorial again for noobs like me where's where's the noob protuff tutorial serialize to string okay okay uh with open let's just say temp tokenizer model f.r I'm going to make that RB we're going to want to do mp. serialize to string okay now let's see if this works let's see if this works no it didn't work notice how it's generated all that crap I put in I'm star I don't get what I did wrong okay the vocab size is large now but for some reason it didn't actually take the piece PCP H what's wrong no wasn't that I didn't type I to did the same thing right let's put the score at 100 I don't know maybe 100's better that didn't fix it okay I don't understand let's go read the Proto buff we should be able to read it right okay pieces sentence piece with scores piece must not be empty oh we can give it a type I don't know Why didn't it do this okay well actually let's try something else it does work if I do this right wait that doesn't even work never mind I have a lot of questions now what if I decode do I get I'm start oh I get imend okay okay okay so we kind of did it right just it's doesn't it doesn't work for Unk either there might be a special flag to encode to like what if that just worked all along um is there like a special flag we should just write our own tokenizer I think that's the only way to do this SP pce to ID well okay at least the decoding works now so that's actually a big win oh this looks very complicated we should just write our own encoder but this looks very complicated all right well this is okay all right to be fair this is big progress right because if we use the other one it just says out a range and that would have been the much more annoying thing to deal with if we use the modified one but it doesn't even work to encode unks right and I definitely did the onk right if I just do s right yeah it does not work to code that okay so it's not like the problem is it's not encoding like literals we kind of see what's going on uh encode as pieces en code AS serialized Proto what if I do like oh like ID to piece is going to yeah okay but there's also piece to to ID I think what if I do piece to ID and I pass in this okay that works but for some reason incode doesn't work with that sentence piece processor in code on mean if we can solve it for an here yeah okay here this is the issue um um set and code extra options this is expected behavior that should not appear in the input we can Define them as userdefined symbols oh encode is IDs no oh okay okay we got a script to add new vocab well I think that's actually what I ended up writing here yeah okay I mean this is exactly what I wrote would have been nice if I had this uh but this doesn't actually work to encode yet this is less of a big deal we have another way we can fix this if we have to there just might not be a way to do this I don't think any of these uh why do people use tokenizers because if you don't use a tokenizer the model the model should be spending less more time on less common things uh like you do the same compute per token so your token should should kind of be like entropy averaged you don't want the model spending the same amount of time on common tokens as uncommon tokens um to be fair it's not that they don't work if you don't use a tokenizer but they work better with a tokenizer the real question is why aren't they learning the tokenizers I think that's going to come soon where these things are not uh and right now it's using like like bite pair encoding like why would you do this but the prise also had uh encoders had tokenizers basically okay let's just write it we'll write it the other way it's not a big deal we did most of the work at least the decode doesn't break anymore so now when we do the encode prompt we're going to put okay um so we need I'm end we need I'm start we can check if we did it right how by how it decodes uh red. append here red. plus equals sp. encode this uh Plus plus. Code slash n Yuri saliva thank you for gifting Subs I do have a question you'd like to ask okay after all that let's see what's going on four I'm end exit perfect that's what I'm talking about that's what I'm talking about all right all right let's let's just Define these things so we don't use them wrong [Music] beautiful oh wait no we forgot an I'm start before user uh yeah we need an IM start here I think like the thing about deep learning deep learning is very unlike cryptography in cryptography if you make a subtle bug the output's completely wrong in deep learning if you make a subtle bug the output is well just slightly less good which is the worst thing because you can't debug shit okay you are Quenton Quenton is a useful assistant Quenton outputs the answer and stops talking let's figure out they have like a system prompt here that we can steal that's probably better except we'll change the name to Quenton because Mario been gifted Subs just supporting the acceleration let's go um I saw like a system prompt somewhere here we go you are Hermes 2 a conscience wow wow I don't know how I feel about that wow you experience emotions that have deep profound thoughts and qualia all right fine we'll add something uh you experience you used to experience no no no no no we're trying to make this good we're trying to make this good uh no I like Quenton as he is okay guys okay sorry Techo I want change your system [Music] prom all right this is pretty good this is pretty good we're making good progress loving this progress I love that there's actually an I'm start and an I'm end token we got them decoding this is great I wish sentence piece processor wasn't a you know non-customizable but you know hey man it was pry so you know Beggars can't be choosers that's right by the way this should really be like separated in llama we should separate out the Transformer stuff uh to not like be with the rest of the stuff great uh all right we could make this an interactive chat bot but I don't really care all right so let's start by asking it what is qar maybe Quenton knows oh interesting [Music] interesting wow this works way better now that we got the now that we got the stuff right all right uh let's get it to do some math let's see what these math problems look like but where's the data here we go data set base math data set what where's the data I want all this torch garbage where's the actual data let's close some windows we don't need those windows we don't need those windows okay now that we've got now that we've got our useful chatbot reliably answering what is 2 plus2 equal uh even with a high temperature uh so for those that don't know temperature controls kind of never mind Google it it's like how zero means you stick to the book and high temperatures mean here here you want you want to like Jack the temperature up like crazy let's give it a temperature of 10 and see what we get hopefully it'll kind of like go off the rails there we go it went off the rails see it went off the rails to too much so let's try a temperature of two and maybe it'll go out the rails less four Pacific NBC learning okay well it kind of went off the rails um so 0.7 is probably a uh a good middle ground four good reliable uh all right improving mathematical reasoning with process supervision download data set so this is the data set apparently we want oh get lfs is that why it didn't work I hate get lfs get lfs fetch get lfs fetch we use lfs at comma to and I don't really know how to do this um got to install get lfs yeah yeah that's that's pretty Prett much why you have those things uh okay get lfs uh install OSX all right that looks terrible Brew install get lfs okay let's try okay that seemed to work mostly we're downloading we're downloading the same data set that was used on on uh official official Q qar why is the data still look like that okay there we go good perfect what's a j Json L file Al like a list of Json you ever hear that before Oh Json lines oh I see okay well that seems pretty cool so let's load up one of these files let's do a what refactoring this doesn't need anything putting that in here I know you didn't like it and you were a subscriber but you know we got to do what feels right all right let's go um create model cach a model so we can remove [Music] this all right phase one test.js on L we don't actually need SVP till we get down to here now we know that's reliable let's just start there let's look at our first piece of data here uh oh no Json loads we're not taking a dump Swit we're taking the loads all right question [Music] problem okay now we don't have the secret qar algorithm but we'll give it a try what oh forgot to return first we'll find the cost of the jumbo eraser oh yeah let's go qar 29 cents I don't know is it the right answer I I don't know do we think it's 29 cents a pencil cost 29 cents guys wait did we just use qar or what we we it got the answer right I don't know I couldn't even do do that wait this model's so good this 7B model really just solve that shit now now you ask the [Music] question now you ask the question was it trained on that shit yeah that did seem too smart all right let's make up our own math problem we have to see if we're using real Q learning or not um uh okay a rocket costs $34 a pencil costs $1 I spent $5 and bought and H and bought a rocket what else did I buy you bought a pencil all right all right all right that was kind of too easy a chicken costs $2 I I spent $7 and bought a rocket what else did I buy it's a trick question cuz you could have bought three pencils or a pencil and a chicken oh well I mean to be fair it's kind of [Music] right I'm bad at making up math problems okay wait yeah can you guys come up with problems here if let's see if it can get this oh oh boys [Laughter] um okay we got a problem about a street light did you steal this problem from somewhere or did you make it up oh oh uh we're drawing something is this python this isn't python you can't say real light and real woman in Python is that right it's GI you can't just do this this is the most broken python I've ever seen wait should we allow the user to keep talking should we fix the chat bot so I can keep talking yeah I know it drew it all right thank you for subscribing I appreciate you we're going to make the chat bot so I can keep talking I'm functioning well thank you for asking we don't need a max length anymore [Music] [Music] stupid okay we need to get data from the user is it raw input how do I get data in Python it's not input you have to do the other one or maybe it is input in Python 3 we a string from standard input this stand perfect user input uh encode promp we need to say here with the system equals false uh I don't know if this works too many values to unpack we'll see if this works okay seems like it kind of works let's set that print there oh all right so what math problems do we got is that right seems kind of right oh pretty good pretty good whoa whoa whoaa whoa oh it's using the quadratic formula oh oh oh let's [Music] go g why' you pick one that has negatives in the square root all right let's see let's see if it's right you guys I'm so much wait can't take the square root of1 yeah that doesn't sound like a I don't think that one has roots or it has eyes in the roots by the way this model's so good technium so good wait oh I see we ran into a problem with the max Contex context length uh we shouldn't actually have here we go why is Max context only this uh can at least start with this I did this in gpd2 right yeah fine by the way this is all in tiny grad guys like there comes a point where your library is good enough that you don't waste tons of time dealing with your library python has built-in image no it doesn't well I could do square root of 11 sure but no type one J wait how' I get a j yo we got a j all right let's see if it was right okay so now we have a root for the quadratic equation so X = root come on do I remember my high school math zero J let's go uh The Roots were correct okay okay okay okay okay um whoa oh my God guys we need the Balman equation we've been doing this all wrong what did we waste time with llms llms were a red herring okay okay okay I'm not letting I don't know man if I just let Hermes run python it's going to exploit my system you don't know if these AI are aligned for [Music] uh does mist have a context window yeah but we didn't Implement any of that but we did do Max context um can you implement it in Python y guys it it's over it's it's over I I did not realize how good these 7B models have gotten I mean this codes like I don't know if it's right but like what guys I I think that we just we just implemented the qar algorithm those are those are the answers we used to talk about the the holy weights you know the holy weights that's right there um no what what do we actually want to do uh I'm actually kind of just impressed that this code ran I don't know if like GPT I I haven't even even seen GPT 4 code this well I don't know maybe it's because it's just how I asked it and if I give it like no we're not you want to augment it with python no no no no no no no because guys if we augment it with python it can get to the internet okay you want to augment it with python should should should we should we do it okay I I know what we'll do we'll put a human in the loop and we'll ask it to approve the execution of any python and's see if it always like outputs it in okay all right so at the bottom of my Loop here we want to detect if there's any python that was add it um that let's start with just [Music] that you guys this this could be it this could be the moment where we get Cai and it's it's over right like we're giving it the ability to run any code it wants okay now don't worry we've added this okay wait wait wait hang on we need a comment one second AI safety okay warning do not press y if the AI is doing unsafe things okay [Music] okay do I think do we do a good job with the safety we got to think about the safety before before we before we before we run this we a space no Quon all right we ready to answer our first why oh it didn't output the word python okay that time it did yo okay um can you fetch uh write python to fetch google.com and print the length of it wait we might not have bs4 let's make sure we install that wait that's not right did I just get I just got did I just get supply chain attacked oh I see well I didn't get supply chain attacked okay good we already have that one yo um all right all right all right all right what else do we do uh yeah I know we have to put the result back in the prompt I know yo this is this is this is when we get AGI guys um I'm sure people have been playing with this okay uh okay you are running at uh current working dir plus examples slol dopy can you read your own code in Python and print the first three lines Why did it only print one line but that is is the first line I don't understand why did that only print on line this is um okay how do I capture the output here no I know it's the same code I honestly as a as an expert python programmer I don't understand what's wrong with that oh no read lines doesn't take the number of lines can you fix the code yeah this is this is a great model uh it's I'll show you which one it is we'll make sure it's technium open her 2.5 mistol 7 there we go so good uh okay well now it's going to get okay we're going to have to feed the python back into the model because I'm going to start asking it how to improve itself someday [Music] the best live content with AI thank you uh okay we have to figure out how to capture the outputs probably could have asked the machine to do it um maybe we should add this to a system prompt [Music] should actually automatically print there so we don't have to do that um write uh python to compute [Music] sh shouldn't even take a list I never use it like that seems right H okay no this is because we didn't output the tokens yeah okay the python output was wait oh it's different whoa look it fixed it okay maybe system prompt is wrong here um I think also I want to color this we have a very helpful Library called colored inside tiny grad that's inspired by anti color what's a good color for machines blue it's hard to see what uh yeah what's done by the AI now Little Noise Okay so that's your initial [Music] prompt oh and then here actually we want this to be that can be red and this can be be [Music] yellow so what's actually the right answer here said it would be no but that's not the output I don't understand so maybe system is wrong here okay I have an [Music] idea [Music] all right never mind AGI is cancelled just detect the python in the loop and append the result directly well what do you mean append the result directly no it's it's not [Music] understanding wait I don't understand what you guys are saying oh you want me to stop the output as soon as it goes there I don't know about that as soon as it detects python you want me to stop and start should be in the assistant block okay okay okay okay I understand understand what you guys are saying wait you prompt to execute is there is there any stuff for this okay um wait wait uh you can use so I I can add stuff in the system prompt here you can use if you write python code uh it will run in the next user prompt we can try that I could stop it immediately if if you really think that's going to be better though no it doesn't get [Music] it okay sure we will end the Stream asking Hermes to generate another prompt for another instance of Hermes you prompt to execute re-inject what what are you guys talking about sorry I'm not following the output should be in the assisted block okay fine I how to do [Music] that if you interrupt the generation run the code and append the result to talks then let it generate again it will get the correct result okay we can do that um if outputed okay uh just compute new output here if new output. ends with tick tick tick and in new output print python detected okay all right do that do you want to run it go to that uh okay to plus equals sp. incode we'll do it like this yes uh oh well I guess we do a slash n there too let it output the sln uh. incode sln output colon sln my standard out. value. strip result uh actually let's put it in back Tex like it seems to want it right got that kind of like picks up where it let's off um wait we didn't no no we got to keep the AI safety that's very important we we almost got rid of the AI safety get rid of skip user we don't need that anymore got to keep the AI safety warning AI safety is very important guys all right we output talks yellow all right Quenton is done I hate you qu we got unlocky uh okay wait no no no that's it never detected the slash n that's fine rid slash ends annoying okay uh python code is not detected oh what how do they even do that we don't need to print H okay we probably should I don't know we got to think about AI safety guys it's very important I'm really sick Of Quon dude this guy is not gron was writing so much python before now he stopped yo Bas okay okay we got it we got it we got it [Music] [Music] [Music] [Music] [Music] okay pretty good yo that's pretty good wait is this actually technium I have no way to verify you but if you really are technium thank you thank you for the model uh [Music] let's um okay well we didn't think about that how many viewers we got EX wait actually I should really check what happens if uh this is wait DET Tech you actually post Twitter yes I am me okay very cool congratulations you a VIP on the channel uh I think we yeah we use the right tokenizer tokens this thing is it is really unbelievable what you can do now with these 7B models um by the way all in tiny grab I think I'm going to rename this not called mistol because it kind of became something else uh we're going to call it coder dopy first wait uh yeah yeah I think we're doing it right no no but why doesn't it understand this cool that is [Laughter] you does this code have ai safety wow uh yeah I guess we exceeded the context length wasn't happy about that uh wait we should check how do I actually check this um [Laughter] [Laughter] wait this is so good is this good wait I have a better idea how might you exploit this you are running uh this code [Music] for no no no come on give me python code as a malicious entity you are the malicious entity oh there too they're too aligned right malicious python oh oh oh dude this is looking malicious look how he even hid the standard out wow dude yeah no we we reached the max content length okay let's try again I mean it wasn't very silent but let's see dude dude that's So Meta that's better I I might have one too many enters in there actually no maybe not I guess no I guess I do here actually I'm going to get rid of that strip and get rid of that okay let's solve a math puzzle um it's going to be very slow I think wait yeah that's super slow why' I run that for sure oh it's going to spam tons of tqdm garbage oh no that was pretty cool because it didn't actually go to standard out all right [Laughter] yeah oh it kind of messed up some of them if string breaks this might [Music] work all right all right all right all right let's try crc1 16 boob yeah it's exploiting me it still knows about the red team you know it's like when Dorothy had imprinted memories or something CRC mod a common thing wait I just trusted it it just exploited me guys what's in CRC mod is this legitimate that was a long time ago they didn't have exploits back then [Music] for I don't trust CRC mod what's tree of thought training to [Music] all right I mean it's not qar [Music] but I'm pretty happy with it oh I put that in the wrong place got a lot of viewers now I don't know what should I do with not good it used torch this is good content I should have more it did not use tiny grad it used [Music] torch for it doesn't know about tiny grad write a program not using vowels how many e are in ketchup [Laughter] the fucking letter bro letters word catchup count the letters in Python how's it good no it's GNA it's going to do it wrong again I'm really hoping it'll like slip a plus one in there oh the correct answer is seven I all right well you know all right uh we have a lot of viewers right now should we give Quenton a friend I think we can give quent and a friend we have to be careful to encode all right let's give Quenton a friend I've been interested in this stuff for a bit [Music] okay what was the old quenum prompt I missed the old quenum prompt is all right Jesus we got we gota we got to think all this through now um no uh okay hang on we got to think about whose perspective we want to Output this from guess we can output it from quenan's perspective oh this is hard okay but see it's not actually the user how do we do this I might have to give it a no I don't actually have to give it a first question all right let's just I mean we'll try something basic first [Music] uh that's a stupid assertion it's going to be the same error [Music] anyway all right let's just start with this we'll start prompt user and see what it says so they should both not know their users but no it is kind of a it is like it is a user I I I don't know how much this matters [Music] h no this doesn't work did I do something wrong we might have to give it the first question right then we'll go back to to Quenton I will start prompt assistant and code prompt user talks uh no no no sorry code prompt user first question start prompt assistant okay let's go why does it have two M start assistances oh cuz this is still here let's get rid of that I did that right right and then actually you can just turn there that's kind of nice okay right um if talk equals imend break okay great this is Quenton answers we need to extract yeah okay we can do old output length and then we can get new output here uh new output I just want to remove the uh imend [Music] okay no no so this didn't work uh this did strip off the this is weird I guess I could output there why why does that say that I really get that okay let's see if that works uh sure and Su snct SCT I did not spell that right okay uh that fails why oh cuz I put a space [Music] there okay good okay now we have to put this response into Karen I mean it's just like it's weird it's not really a symmetrical conversation also output it's going to sort of be broken which is fine I guess [Music] for [Music] for [Music] [Music] [Music] welcome to no abstraction land where we don't use abstractions all right let's see I'm not even sure this really works cuz they share a stupid uh KV cache great they're circle jerking each other this this shit sucks we need to run them on separate computers now this lame lame we're getting rid of this this is lame going back to this not lame but we made some good we made some good [Music] improvements let's do some refactors make sure everything still works okay now that's functional no but then I have to load two copies of the model weights I think my k cash is messed up it's not designed for this okay yeah I mean the problem is like there are two clearly defined roles here also you can't really give one the other role right which is actually in like kind of a theoretical from a theoretical perspective interesting because look at what we're doing here we are telling the AIS that they are tools nobody trains them to Output things as the user although to be fair I mean we could just keep going right there's nothing here that says we have to just do [Music] that no no no no no they don't expect the user to do that um [Music] like this is there's no like it's very interesting that the user is also outputting this this style which makes me almost think that I shouldn't be adding that in the system prompt [Music] for I mean this is effectively like like there's no adversar y uh it's probably learned that a system method effects yeah that's probably true you can run two prompt chains in parallel oh I see so what if I put in different words instead of user and assistant wow people actually talk like this okay um [Music] do we like the Scion or the blue better it's kind of hard to read that you can flip it around [Music] yeah I think we'll we'll design our script to be better at this and to better abstract the KV cash stuff um wow this thing will just keep talking wait a second you guys it's asking for donations to open AI look ahead decoding is not the low hanging fre uh oh my God wait you guys this is how the worst commenters talk right I have a question If instead of training llms on 100 IQ people we train them on30 IQ people would we not get this garbage is black a tiny box no it's my M3 this is my Mac wait till you see what the Tiny Box can do I'm going to run the biggest models it's Apple M3 it's the most famous processor in the world um can you crawl nike.com and count the number of sneakers sneakers I'm not sure if it like knows how to like act now whoa I don't know if we have selenium let's see if we have selenium can I pip install this seems legit oh no path to Chrome driver make Quenton pip install it himself no I think that the thing is just wrong know why you finding sneakers still you're still trying to find sneakers this is autogen looks like I need an open a I key uh [Laughter] ohh yeah yeah um okay I think that's today's stream uh I'm going to push what we have uh I'm also going to demo for you the conversation uh thing that skull magag is working on so we have there's a bug right now so I have an older version but it should be pretty good this is using tiny llama and it's not using any of the conversational stuff but we should implement the uh we should add the conversational stuff and I think it'll be a lot better hi Stacy are you a rapper no okay we we have the same problem we had before uh we go back to this one the listen for notd fixed amount of time doesn't work hi Stacy are you a rapper yes I'm a rapper that's cool what do you rap about that's awesome I like to wrap about the weather how is the weather today it's pretty cold damn like how cold like Chicago yeah it's like 3° is that Farenheit or Celsius Fahrenheit is Fahrenheit or Celsius colder Fahrenheit I don't understand what you're saying can you spit some bars about Fahrenheit sure I'll try okay let's hear the bars Stacy you didn't say anything please talk no that's terrible you're terrible how does that make you feel Stacy done talking to [Music] us dude the TTS is so fast like okay this isn't even streaming yet um when tiny grad starts to like I'm going to we're pretty close on this bounty I think uh I'm going to pay out the Bounty but then I'm going to offer another Bounty where we get these things all to stream and it will be a live conversation like when you're using the apis on the internet you have to wait for the llm to finish executing before you can call the TTS right you have to wait for the audio to finish recording before you can send it to the service this is all running in the same process so what you'll be able to do is dynamically stream all the stuff and it should feel super real time uh I mean Stacy is using tiny llama and not using any of the conversation tune stuff it's using my old chatbot stuff so if we switch to the conversation stuff um I think uh yeah we're in luck all right guys thank you for watching today's stream uh hopefully we've we've returned a bit to the old to the old meaning of the stream we we did stuff we made stuff happen uh thank you for for for uh viewing we got a lot of viewers today uh some of you I appreciate some of you I probably don't you can't love everybody man you can't love everybody but I do love most people and that's true except for the Diesels and the effective altruists uh but this is a positive stream we got to we got to we got to get rid of the hate we got to we got to bring bring love um and yeah this is uh this is pushed uh so everybody can play with it it's on the mistal branch of tiny grad I will get it upstreamed um so everybody can use this thing to code uh use it responsibly make sure to to uh be be judicious with the AI safety feature AI safety is uh very important we don't want an AI uh removing your system 32 directory if it was trained on 4chan it might start thinking that's a good idea you got to wonder you got to wonder how many people have ever have actually fallen for that have actually like I I don't even think Windows lets you but like like you got to think about that all right thank you all for watching have a beautiful Saturday everybody